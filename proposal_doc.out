\BOOKMARK [0][-]{chapter.1}{Background/History of the Study}{}% 1
\BOOKMARK [1][-]{section.1.1}{Background}{chapter.1}% 2
\BOOKMARK [1][-]{section.1.2}{Recurrent Neural Network Components}{chapter.1}% 3
\BOOKMARK [2][-]{subsection.1.2.1}{Overview}{section.1.2}% 4
\BOOKMARK [2][-]{subsection.1.2.2}{Gated Recurrent Unit}{section.1.2}% 5
\BOOKMARK [1][-]{section.1.3}{Sequence to Sequence and Translation}{chapter.1}% 6
\BOOKMARK [2][-]{subsection.1.3.1}{Word Embeddings}{section.1.3}% 7
\BOOKMARK [2][-]{subsection.1.3.2}{Corpus}{section.1.3}% 8
\BOOKMARK [2][-]{subsection.1.3.3}{Training and Evaluation}{section.1.3}% 9
\BOOKMARK [2][-]{subsection.1.3.4}{Input Tokens}{section.1.3}% 10
\BOOKMARK [2][-]{subsection.1.3.5}{Encoder}{section.1.3}% 11
\BOOKMARK [2][-]{subsection.1.3.6}{Decoder}{section.1.3}% 12
\BOOKMARK [2][-]{subsection.1.3.7}{Output Tokens}{section.1.3}% 13
\BOOKMARK [2][-]{subsection.1.3.8}{Loss and Accuracy During Training}{section.1.3}% 14
\BOOKMARK [2][-]{subsection.1.3.9}{Attention Mechanism}{section.1.3}% 15
\BOOKMARK [2][-]{subsection.1.3.10}{Sequence to Sequence Chatbot}{section.1.3}% 16
\BOOKMARK [0][-]{chapter.2}{Transformers and The Generative Pre-training Transformer 2}{}% 17
\BOOKMARK [1][-]{section.2.1}{Transformer and Attention}{chapter.2}% 18
\BOOKMARK [2][-]{subsection.2.1.1}{Byte Pair Encoding}{section.2.1}% 19
\BOOKMARK [2][-]{subsection.2.1.2}{Attention}{section.2.1}% 20
\BOOKMARK [2][-]{subsection.2.1.3}{Scaled Dot-Product Attention}{section.2.1}% 21
\BOOKMARK [2][-]{subsection.2.1.4}{Decoder}{section.2.1}% 22
\BOOKMARK [2][-]{subsection.2.1.5}{Input - Positional Encoding}{section.2.1}% 23
\BOOKMARK [2][-]{subsection.2.1.6}{Output - Feed Forward Network}{section.2.1}% 24
\BOOKMARK [2][-]{subsection.2.1.7}{Visualization - Transformer}{section.2.1}% 25
\BOOKMARK [1][-]{section.2.2}{The Generative Pre-training Transformer 2 Model}{chapter.2}% 26
\BOOKMARK [2][-]{subsection.2.2.1}{Pre-Training}{section.2.2}% 27
\BOOKMARK [2][-]{subsection.2.2.2}{General}{section.2.2}% 28
\BOOKMARK [2][-]{subsection.2.2.3}{Corpus}{section.2.2}% 29
\BOOKMARK [2][-]{subsection.2.2.4}{Releases}{section.2.2}% 30
\BOOKMARK [2][-]{subsection.2.2.5}{Application Details}{section.2.2}% 31
\BOOKMARK [2][-]{subsection.2.2.6}{Visualization - GPT2}{section.2.2}% 32
\BOOKMARK [0][-]{chapter.3}{Experimental Design and Setup}{}% 33
\BOOKMARK [1][-]{section.3.1}{Approach to the Study}{chapter.3}% 34
\BOOKMARK [1][-]{section.3.2}{Model Overview}{chapter.3}% 35
\BOOKMARK [1][-]{section.3.3}{Setup}{chapter.3}% 36
\BOOKMARK [2][-]{subsection.3.3.1}{Graphical Processing Unit vs. Central Processing Unit}{section.3.3}% 37
\BOOKMARK [2][-]{subsection.3.3.2}{Raspberry Pi}{section.3.3}% 38
\BOOKMARK [2][-]{subsection.3.3.3}{Tensorflow vs. Pytorch}{section.3.3}% 39
\BOOKMARK [2][-]{subsection.3.3.4}{Speech and Speech To Text}{section.3.3}% 40
\BOOKMARK [2][-]{subsection.3.3.5}{Corpus Considerations}{section.3.3}% 41
\BOOKMARK [1][-]{section.3.4}{ARMv7 Build/Compile}{chapter.3}% 42
\BOOKMARK [2][-]{subsection.3.4.1}{Pytorch `torch' Library 1.1.0 For ARMv7}{section.3.4}% 43
\BOOKMARK [2][-]{subsection.3.4.2}{Pytorch `torch' Library 1.4.0 For ARMv7}{section.3.4}% 44
\BOOKMARK [2][-]{subsection.3.4.3}{Docker Container `tensorflow-model-server' For ARMv7}{section.3.4}% 45
\BOOKMARK [0][-]{chapter.4}{Experimental Results}{}% 46
\BOOKMARK [1][-]{section.4.1}{Chatbot - Gated Recurrent Unit Model}{chapter.4}% 47
\BOOKMARK [1][-]{section.4.2}{Smart Speaker - Gated Recurrent Unit Model}{chapter.4}% 48
\BOOKMARK [1][-]{section.4.3}{Chatbot - Transformer Model with Persona Corpus}{chapter.4}% 49
\BOOKMARK [2][-]{subsection.4.3.1}{Training}{section.4.3}% 50
\BOOKMARK [1][-]{section.4.4}{Smart Speaker - Transformer Model with Persona Corpus}{chapter.4}% 51
\BOOKMARK [1][-]{section.4.5}{Chatbot - Transformer Model with Movie Corpus}{chapter.4}% 52
\BOOKMARK [1][-]{section.4.6}{Smart Speaker - Transformer Model with Movie Corpus}{chapter.4}% 53
\BOOKMARK [1][-]{section.4.7}{Chatbot - Generative Pre-training Transformer 2 Model}{chapter.4}% 54
\BOOKMARK [2][-]{subsection.4.7.1}{Context Experiment}{section.4.7}% 55
\BOOKMARK [2][-]{subsection.4.7.2}{History Experiment}{section.4.7}% 56
\BOOKMARK [2][-]{subsection.4.7.3}{Artificial Intelligence Markup Language Experiment}{section.4.7}% 57
\BOOKMARK [2][-]{subsection.4.7.4}{Program Launching}{section.4.7}% 58
\BOOKMARK [2][-]{subsection.4.7.5}{Overall}{section.4.7}% 59
\BOOKMARK [1][-]{section.4.8}{Smart Speaker - Generative Pre-training Transformer 2 Model}{chapter.4}% 60
\BOOKMARK [0][-]{chapter.5}{Conclusions}{}% 61
\BOOKMARK [1][-]{section.5.1}{GRU vs. Transformer}{chapter.5}% 62
\BOOKMARK [1][-]{section.5.2}{Smaller Chatbot Learning}{chapter.5}% 63
\BOOKMARK [1][-]{section.5.3}{Word Usage}{chapter.5}% 64
\BOOKMARK [1][-]{section.5.4}{Sentence Usage}{chapter.5}% 65
\BOOKMARK [1][-]{section.5.5}{Turing Test}{chapter.5}% 66
\BOOKMARK [1][-]{section.5.6}{Winograd Schema}{chapter.5}% 67
\BOOKMARK [0][-]{appendix.A}{Further Installations}{}% 68
\BOOKMARK [1][-]{section.A.1}{Generative Pre-training Transformer 2 - Large Model}{appendix.A}% 69
\BOOKMARK [2][-]{subsection.A.1.1}{Context Experiment}{section.A.1}% 70
\BOOKMARK [2][-]{subsection.A.1.2}{History Experiment}{section.A.1}% 71
\BOOKMARK [2][-]{subsection.A.1.3}{Artificial Intelligence Markup Language Experiment}{section.A.1}% 72
\BOOKMARK [0][-]{appendix.B}{Abbreviations}{}% 73
