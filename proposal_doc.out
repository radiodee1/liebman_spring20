\BOOKMARK [0][-]{chapter.1}{Background/History of the Study}{}% 1
\BOOKMARK [1][-]{section.1.1}{Background}{chapter.1}% 2
\BOOKMARK [2][-]{subsection.1.1.1}{Recurrent Neural Network and Transformer}{section.1.1}% 3
\BOOKMARK [2][-]{subsection.1.1.2}{Pre Trained Language Models}{section.1.1}% 4
\BOOKMARK [1][-]{section.1.2}{Outline}{chapter.1}% 5
\BOOKMARK [1][-]{section.1.3}{Goals For This Thesis}{chapter.1}% 6
\BOOKMARK [0][-]{chapter.2}{Recurrent Neural Network}{}% 7
\BOOKMARK [1][-]{section.2.1}{Recurrent Neural Network Components}{chapter.2}% 8
\BOOKMARK [2][-]{subsection.2.1.1}{Hidden Values}{section.2.1}% 9
\BOOKMARK [2][-]{subsection.2.1.2}{Simple RNN}{section.2.1}% 10
\BOOKMARK [2][-]{subsection.2.1.3}{Gated Recurrent Unit}{section.2.1}% 11
\BOOKMARK [1][-]{section.2.2}{Sequence-to-Sequence and Translation}{chapter.2}% 12
\BOOKMARK [2][-]{subsection.2.2.1}{Word Embeddings}{section.2.2}% 13
\BOOKMARK [2][-]{subsection.2.2.2}{Corpus}{section.2.2}% 14
\BOOKMARK [2][-]{subsection.2.2.3}{Training and Evaluation}{section.2.2}% 15
\BOOKMARK [2][-]{subsection.2.2.4}{Input Tokens}{section.2.2}% 16
\BOOKMARK [2][-]{subsection.2.2.5}{Encoder}{section.2.2}% 17
\BOOKMARK [2][-]{subsection.2.2.6}{Decoder}{section.2.2}% 18
\BOOKMARK [2][-]{subsection.2.2.7}{Output Tokens}{section.2.2}% 19
\BOOKMARK [2][-]{subsection.2.2.8}{Loss and Accuracy During Training}{section.2.2}% 20
\BOOKMARK [2][-]{subsection.2.2.9}{Attention Mechanism}{section.2.2}% 21
\BOOKMARK [2][-]{subsection.2.2.10}{Batching Problems}{section.2.2}% 22
\BOOKMARK [2][-]{subsection.2.2.11}{Exploding or Vanishing Gradients}{section.2.2}% 23
\BOOKMARK [2][-]{subsection.2.2.12}{Sequence-to-Sequence Chatbot}{section.2.2}% 24
\BOOKMARK [0][-]{chapter.3}{Transformers and the Generative Pre-Training 2}{}% 25
\BOOKMARK [1][-]{section.3.1}{Transformer and Attention}{chapter.3}% 26
\BOOKMARK [2][-]{subsection.3.1.1}{Byte Pair Encoding}{section.3.1}% 27
\BOOKMARK [2][-]{subsection.3.1.2}{Attention}{section.3.1}% 28
\BOOKMARK [2][-]{subsection.3.1.3}{Encoder - Scaled Dot-Product Attention}{section.3.1}% 29
\BOOKMARK [2][-]{subsection.3.1.4}{Decoder Attention I - ``Key'' and ``Value''}{section.3.1}% 30
\BOOKMARK [2][-]{subsection.3.1.5}{Decoder Attention II - ``Query''}{section.3.1}% 31
\BOOKMARK [2][-]{subsection.3.1.6}{Decoder Attention II - Masking}{section.3.1}% 32
\BOOKMARK [2][-]{subsection.3.1.7}{Input - Positional Encoding}{section.3.1}% 33
\BOOKMARK [2][-]{subsection.3.1.8}{Output - Feed Forward Network}{section.3.1}% 34
\BOOKMARK [2][-]{subsection.3.1.9}{Visualization - Transformer}{section.3.1}% 35
\BOOKMARK [1][-]{section.3.2}{The Generative Pre-Training 2 Model}{chapter.3}% 36
\BOOKMARK [2][-]{subsection.3.2.1}{Pre-Training}{section.3.2}% 37
\BOOKMARK [2][-]{subsection.3.2.2}{General}{section.3.2}% 38
\BOOKMARK [2][-]{subsection.3.2.3}{Training}{section.3.2}% 39
\BOOKMARK [2][-]{subsection.3.2.4}{Inference}{section.3.2}% 40
\BOOKMARK [2][-]{subsection.3.2.5}{Corpus}{section.3.2}% 41
\BOOKMARK [2][-]{subsection.3.2.6}{Releases}{section.3.2}% 42
\BOOKMARK [2][-]{subsection.3.2.7}{Application Details}{section.3.2}% 43
\BOOKMARK [2][-]{subsection.3.2.8}{Visualization - GPT2}{section.3.2}% 44
\BOOKMARK [0][-]{chapter.4}{Experimental Design and Setup}{}% 45
\BOOKMARK [1][-]{section.4.1}{Approach to the Study}{chapter.4}% 46
\BOOKMARK [1][-]{section.4.2}{Hardware Installation Overview}{chapter.4}% 47
\BOOKMARK [1][-]{section.4.3}{Setup}{chapter.4}% 48
\BOOKMARK [2][-]{subsection.4.3.1}{Graphical Processing Unit vs. Central Processing Unit}{section.4.3}% 49
\BOOKMARK [2][-]{subsection.4.3.2}{Raspberry Pi}{section.4.3}% 50
\BOOKMARK [2][-]{subsection.4.3.3}{Jetson Nano}{section.4.3}% 51
\BOOKMARK [2][-]{subsection.4.3.4}{Reply Time}{section.4.3}% 52
\BOOKMARK [2][-]{subsection.4.3.5}{Tensorflow vs. Pytorch}{section.4.3}% 53
\BOOKMARK [2][-]{subsection.4.3.6}{Speech and Speech To Text}{section.4.3}% 54
\BOOKMARK [2][-]{subsection.4.3.7}{Corpus Considerations}{section.4.3}% 55
\BOOKMARK [2][-]{subsection.4.3.8}{Chatbot vs. Smart Speaker}{section.4.3}% 56
\BOOKMARK [1][-]{section.4.4}{ARMv7 Build/Compile}{chapter.4}% 57
\BOOKMARK [2][-]{subsection.4.4.1}{Pytorch ``torch'' Library 1.1.0 For ARMv7}{section.4.4}% 58
\BOOKMARK [2][-]{subsection.4.4.2}{Pytorch ``torch'' Library 1.4.0 For ARMv7}{section.4.4}% 59
\BOOKMARK [2][-]{subsection.4.4.3}{Docker Container ``tensorflow-model-server'' For ARMv7}{section.4.4}% 60
\BOOKMARK [0][-]{chapter.5}{Experimental Results - Raspberry Pi}{}% 61
\BOOKMARK [1][-]{section.5.1}{Experiments - Installations}{chapter.5}% 62
\BOOKMARK [2][-]{subsection.5.1.1}{Questions}{section.5.1}% 63
\BOOKMARK [2][-]{subsection.5.1.2}{Checklist}{section.5.1}% 64
\BOOKMARK [1][-]{section.5.2}{Chatbot - Gated Recurrent Unit Model}{chapter.5}% 65
\BOOKMARK [2][-]{subsection.5.2.1}{Questions}{section.5.2}% 66
\BOOKMARK [2][-]{subsection.5.2.2}{Checklist}{section.5.2}% 67
\BOOKMARK [1][-]{section.5.3}{Smart Speaker - Gated Recurrent Unit Model}{chapter.5}% 68
\BOOKMARK [1][-]{section.5.4}{Chatbot - Transformer Model with Persona Corpus}{chapter.5}% 69
\BOOKMARK [2][-]{subsection.5.4.1}{Questions}{section.5.4}% 70
\BOOKMARK [2][-]{subsection.5.4.2}{Checklist}{section.5.4}% 71
\BOOKMARK [1][-]{section.5.5}{Smart Speaker - Transformer Model with Persona Corpus}{chapter.5}% 72
\BOOKMARK [1][-]{section.5.6}{Chatbot - Transformer Model with Movie Corpus}{chapter.5}% 73
\BOOKMARK [2][-]{subsection.5.6.1}{Questions}{section.5.6}% 74
\BOOKMARK [2][-]{subsection.5.6.2}{Checklist}{section.5.6}% 75
\BOOKMARK [1][-]{section.5.7}{Smart Speaker - Transformer Model with Movie Corpus}{chapter.5}% 76
\BOOKMARK [1][-]{section.5.8}{Chatbot - Generative Pre-Training 2 Model}{chapter.5}% 77
\BOOKMARK [2][-]{subsection.5.8.1}{Context Experiment}{section.5.8}% 78
\BOOKMARK [2][-]{subsection.5.8.2}{GPT2 Fact Sheet}{section.5.8}% 79
\BOOKMARK [2][-]{subsection.5.8.3}{History Experiment}{section.5.8}% 80
\BOOKMARK [2][-]{subsection.5.8.4}{Program Launching}{section.5.8}% 81
\BOOKMARK [2][-]{subsection.5.8.5}{Summary}{section.5.8}% 82
\BOOKMARK [2][-]{subsection.5.8.6}{Questions}{section.5.8}% 83
\BOOKMARK [2][-]{subsection.5.8.7}{Checklist}{section.5.8}% 84
\BOOKMARK [1][-]{section.5.9}{Smart Speaker - Generative Pre-Training 2 Model}{chapter.5}% 85
\BOOKMARK [0][-]{chapter.6}{Further Installations}{}% 86
\BOOKMARK [1][-]{section.6.1}{Generative Pre-Training 2 - X Large Model}{chapter.6}% 87
\BOOKMARK [2][-]{subsection.6.1.1}{Questions}{section.6.1}% 88
\BOOKMARK [2][-]{subsection.6.1.2}{Checklist}{section.6.1}% 89
\BOOKMARK [2][-]{subsection.6.1.3}{Context Experiment}{section.6.1}% 90
\BOOKMARK [2][-]{subsection.6.1.4}{History Experiment}{section.6.1}% 91
\BOOKMARK [2][-]{subsection.6.1.5}{Artificial Intelligence Markup Language Experiment}{section.6.1}% 92
\BOOKMARK [2][-]{subsection.6.1.6}{User Name Experiment}{section.6.1}% 93
\BOOKMARK [2][-]{subsection.6.1.7}{Internet Search Experiment}{section.6.1}% 94
\BOOKMARK [1][-]{section.6.2}{Generative Pre-Training 2 - Jetson Nano}{chapter.6}% 95
\BOOKMARK [0][-]{chapter.7}{Results and Observations}{}% 96
\BOOKMARK [1][-]{section.7.1}{GRU vs. Transformer}{chapter.7}% 97
\BOOKMARK [1][-]{section.7.2}{Turing Test}{chapter.7}% 98
\BOOKMARK [1][-]{section.7.3}{Word and Sentence Comparisons}{chapter.7}% 99
\BOOKMARK [2][-]{subsection.7.3.1}{Word Usage}{section.7.3}% 100
\BOOKMARK [2][-]{subsection.7.3.2}{Sentence Usage}{section.7.3}% 101
\BOOKMARK [2][-]{subsection.7.3.3}{Maximum Sentence Values}{section.7.3}% 102
\BOOKMARK [1][-]{section.7.4}{Training and Lists}{chapter.7}% 103
\BOOKMARK [0][-]{chapter.8}{Further Research and Readings}{}% 104
\BOOKMARK [1][-]{section.8.1}{Questions and Further Research}{chapter.8}% 105
\BOOKMARK [1][-]{section.8.2}{Winograd Schema}{chapter.8}% 106
\BOOKMARK [1][-]{section.8.3}{Generative Pre-Training 3}{chapter.8}% 107
