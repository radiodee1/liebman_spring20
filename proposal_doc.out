\BOOKMARK [0][-]{chapter.1}{Background/History of the Study}{}% 1
\BOOKMARK [1][-]{section.1.1}{Background}{chapter.1}% 2
\BOOKMARK [2][-]{subsection.1.1.1}{Recurrent Neural Network and Transformer}{section.1.1}% 3
\BOOKMARK [2][-]{subsection.1.1.2}{Pre Trained Language Models}{section.1.1}% 4
\BOOKMARK [1][-]{section.1.2}{Recurrent Neural Network Components}{chapter.1}% 5
\BOOKMARK [2][-]{subsection.1.2.1}{Hidden Values}{section.1.2}% 6
\BOOKMARK [2][-]{subsection.1.2.2}{Simple RNN}{section.1.2}% 7
\BOOKMARK [2][-]{subsection.1.2.3}{Gated Recurrent Unit}{section.1.2}% 8
\BOOKMARK [1][-]{section.1.3}{Sequence-to-Sequence and Translation}{chapter.1}% 9
\BOOKMARK [2][-]{subsection.1.3.1}{Word Embeddings}{section.1.3}% 10
\BOOKMARK [2][-]{subsection.1.3.2}{Corpus}{section.1.3}% 11
\BOOKMARK [2][-]{subsection.1.3.3}{Training and Evaluation}{section.1.3}% 12
\BOOKMARK [2][-]{subsection.1.3.4}{Input Tokens}{section.1.3}% 13
\BOOKMARK [2][-]{subsection.1.3.5}{Encoder}{section.1.3}% 14
\BOOKMARK [2][-]{subsection.1.3.6}{Decoder}{section.1.3}% 15
\BOOKMARK [2][-]{subsection.1.3.7}{Output Tokens}{section.1.3}% 16
\BOOKMARK [2][-]{subsection.1.3.8}{Loss and Accuracy During Training}{section.1.3}% 17
\BOOKMARK [2][-]{subsection.1.3.9}{Attention Mechanism}{section.1.3}% 18
\BOOKMARK [2][-]{subsection.1.3.10}{Batching Problems}{section.1.3}% 19
\BOOKMARK [2][-]{subsection.1.3.11}{Exploding or Vanishing Gradients}{section.1.3}% 20
\BOOKMARK [2][-]{subsection.1.3.12}{Sequence-to-Sequence Chatbot}{section.1.3}% 21
\BOOKMARK [0][-]{chapter.2}{Transformers and the Generative Pre-Training 2 Transformer}{}% 22
\BOOKMARK [1][-]{section.2.1}{Transformer and Attention}{chapter.2}% 23
\BOOKMARK [2][-]{subsection.2.1.1}{Byte Pair Encoding}{section.2.1}% 24
\BOOKMARK [2][-]{subsection.2.1.2}{Attention}{section.2.1}% 25
\BOOKMARK [2][-]{subsection.2.1.3}{Encoder - Scaled Dot-Product Attention}{section.2.1}% 26
\BOOKMARK [2][-]{subsection.2.1.4}{Decoder Attention I - `Key' and `Value'}{section.2.1}% 27
\BOOKMARK [2][-]{subsection.2.1.5}{Decoder Attention II - `Query'}{section.2.1}% 28
\BOOKMARK [2][-]{subsection.2.1.6}{Decoder Attention II - Masking}{section.2.1}% 29
\BOOKMARK [2][-]{subsection.2.1.7}{Input - Positional Encoding}{section.2.1}% 30
\BOOKMARK [2][-]{subsection.2.1.8}{Output - Feed Forward Network}{section.2.1}% 31
\BOOKMARK [2][-]{subsection.2.1.9}{Visualization - Transformer}{section.2.1}% 32
\BOOKMARK [1][-]{section.2.2}{The Generative Pre-Training 2 Model}{chapter.2}% 33
\BOOKMARK [2][-]{subsection.2.2.1}{Pre-Training}{section.2.2}% 34
\BOOKMARK [2][-]{subsection.2.2.2}{General}{section.2.2}% 35
\BOOKMARK [2][-]{subsection.2.2.3}{Training}{section.2.2}% 36
\BOOKMARK [2][-]{subsection.2.2.4}{Inference}{section.2.2}% 37
\BOOKMARK [2][-]{subsection.2.2.5}{Corpus}{section.2.2}% 38
\BOOKMARK [2][-]{subsection.2.2.6}{Releases}{section.2.2}% 39
\BOOKMARK [2][-]{subsection.2.2.7}{Application Details}{section.2.2}% 40
\BOOKMARK [2][-]{subsection.2.2.8}{Visualization - GPT2}{section.2.2}% 41
\BOOKMARK [0][-]{chapter.3}{Experimental Design and Setup}{}% 42
\BOOKMARK [1][-]{section.3.1}{Approach to the Study}{chapter.3}% 43
\BOOKMARK [1][-]{section.3.2}{Hardware Installation Overview}{chapter.3}% 44
\BOOKMARK [1][-]{section.3.3}{Setup}{chapter.3}% 45
\BOOKMARK [2][-]{subsection.3.3.1}{Graphical Processing Unit vs. Central Processing Unit}{section.3.3}% 46
\BOOKMARK [2][-]{subsection.3.3.2}{Raspberry Pi}{section.3.3}% 47
\BOOKMARK [2][-]{subsection.3.3.3}{Jetson Nano}{section.3.3}% 48
\BOOKMARK [2][-]{subsection.3.3.4}{Reply Time}{section.3.3}% 49
\BOOKMARK [2][-]{subsection.3.3.5}{Tensorflow vs. Pytorch}{section.3.3}% 50
\BOOKMARK [2][-]{subsection.3.3.6}{Speech and Speech To Text}{section.3.3}% 51
\BOOKMARK [2][-]{subsection.3.3.7}{Corpus Considerations}{section.3.3}% 52
\BOOKMARK [2][-]{subsection.3.3.8}{Chatbot vs. Smart Speaker}{section.3.3}% 53
\BOOKMARK [1][-]{section.3.4}{ARMv7 Build/Compile}{chapter.3}% 54
\BOOKMARK [2][-]{subsection.3.4.1}{Pytorch `torch' Library 1.1.0 For ARMv7}{section.3.4}% 55
\BOOKMARK [2][-]{subsection.3.4.2}{Pytorch `torch' Library 1.4.0 For ARMv7}{section.3.4}% 56
\BOOKMARK [2][-]{subsection.3.4.3}{Docker Container `tensorflow-model-server' For ARMv7}{section.3.4}% 57
\BOOKMARK [0][-]{chapter.4}{Experimental Results - Raspberry Pi}{}% 58
\BOOKMARK [1][-]{section.4.1}{Experiments - Installations}{chapter.4}% 59
\BOOKMARK [2][-]{subsection.4.1.1}{Questions}{section.4.1}% 60
\BOOKMARK [2][-]{subsection.4.1.2}{Checklist}{section.4.1}% 61
\BOOKMARK [1][-]{section.4.2}{Chatbot - Gated Recurrent Unit Model}{chapter.4}% 62
\BOOKMARK [2][-]{subsection.4.2.1}{Questions}{section.4.2}% 63
\BOOKMARK [2][-]{subsection.4.2.2}{Checklist}{section.4.2}% 64
\BOOKMARK [1][-]{section.4.3}{Smart Speaker - Gated Recurrent Unit Model}{chapter.4}% 65
\BOOKMARK [1][-]{section.4.4}{Chatbot - Transformer Model with Persona Corpus}{chapter.4}% 66
\BOOKMARK [2][-]{subsection.4.4.1}{Questions}{section.4.4}% 67
\BOOKMARK [2][-]{subsection.4.4.2}{Checklist}{section.4.4}% 68
\BOOKMARK [1][-]{section.4.5}{Smart Speaker - Transformer Model with Persona Corpus}{chapter.4}% 69
\BOOKMARK [1][-]{section.4.6}{Chatbot - Transformer Model with Movie Corpus}{chapter.4}% 70
\BOOKMARK [2][-]{subsection.4.6.1}{Questions}{section.4.6}% 71
\BOOKMARK [2][-]{subsection.4.6.2}{Checklist}{section.4.6}% 72
\BOOKMARK [1][-]{section.4.7}{Smart Speaker - Transformer Model with Movie Corpus}{chapter.4}% 73
\BOOKMARK [1][-]{section.4.8}{Chatbot - Generative Pre-Training 2 Model}{chapter.4}% 74
\BOOKMARK [2][-]{subsection.4.8.1}{Context Experiment}{section.4.8}% 75
\BOOKMARK [2][-]{subsection.4.8.2}{GPT2 Fact Sheet}{section.4.8}% 76
\BOOKMARK [2][-]{subsection.4.8.3}{History Experiment}{section.4.8}% 77
\BOOKMARK [2][-]{subsection.4.8.4}{Program Launching}{section.4.8}% 78
\BOOKMARK [2][-]{subsection.4.8.5}{Summary}{section.4.8}% 79
\BOOKMARK [2][-]{subsection.4.8.6}{Questions}{section.4.8}% 80
\BOOKMARK [2][-]{subsection.4.8.7}{Checklist}{section.4.8}% 81
\BOOKMARK [1][-]{section.4.9}{Smart Speaker - Generative Pre-Training 2 Model}{chapter.4}% 82
\BOOKMARK [0][-]{chapter.5}{Further Installations}{}% 83
\BOOKMARK [1][-]{section.5.1}{Generative Pre-Training 2 - X Large Model}{chapter.5}% 84
\BOOKMARK [2][-]{subsection.5.1.1}{Questions}{section.5.1}% 85
\BOOKMARK [2][-]{subsection.5.1.2}{Checklist}{section.5.1}% 86
\BOOKMARK [2][-]{subsection.5.1.3}{Context Experiment}{section.5.1}% 87
\BOOKMARK [2][-]{subsection.5.1.4}{History Experiment}{section.5.1}% 88
\BOOKMARK [2][-]{subsection.5.1.5}{Artificial Intelligence Markup Language Experiment}{section.5.1}% 89
\BOOKMARK [2][-]{subsection.5.1.6}{User Name Experiment}{section.5.1}% 90
\BOOKMARK [2][-]{subsection.5.1.7}{Internet Search Experiment}{section.5.1}% 91
\BOOKMARK [1][-]{section.5.2}{Generative Pre-Training 2 - Jetson Nano}{chapter.5}% 92
\BOOKMARK [0][-]{chapter.6}{Results and Observations}{}% 93
\BOOKMARK [1][-]{section.6.1}{Goals For This Thesis}{chapter.6}% 94
\BOOKMARK [1][-]{section.6.2}{GRU vs. Transformer}{chapter.6}% 95
\BOOKMARK [1][-]{section.6.3}{Turing Test}{chapter.6}% 96
\BOOKMARK [1][-]{section.6.4}{Word and Sentence Comparisons}{chapter.6}% 97
\BOOKMARK [2][-]{subsection.6.4.1}{Word Usage}{section.6.4}% 98
\BOOKMARK [2][-]{subsection.6.4.2}{Sentence Usage}{section.6.4}% 99
\BOOKMARK [2][-]{subsection.6.4.3}{Maximum Sentence Values}{section.6.4}% 100
\BOOKMARK [1][-]{section.6.5}{Training and Lists}{chapter.6}% 101
\BOOKMARK [0][-]{chapter.7}{Further Research and Readings}{}% 102
\BOOKMARK [1][-]{section.7.1}{Questions and Further Research}{chapter.7}% 103
\BOOKMARK [1][-]{section.7.2}{Winograd Schema}{chapter.7}% 104
\BOOKMARK [1][-]{section.7.3}{Generative Pre-Training 3}{chapter.7}% 105
\BOOKMARK [0][-]{appendix.A}{Abbreviations}{}% 106
