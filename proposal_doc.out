\BOOKMARK [0][-]{chapter.1}{Background/History of the Study}{}% 1
\BOOKMARK [1][-]{section.1.1}{Background}{chapter.1}% 2
\BOOKMARK [1][-]{section.1.2}{Recurrent Neural Network Components}{chapter.1}% 3
\BOOKMARK [2][-]{subsection.1.2.1}{Overview}{section.1.2}% 4
\BOOKMARK [2][-]{subsection.1.2.2}{Gated Recurrent Unit}{section.1.2}% 5
\BOOKMARK [1][-]{section.1.3}{Sequence to Sequence and Translation}{chapter.1}% 6
\BOOKMARK [2][-]{subsection.1.3.1}{Word Embeddings}{section.1.3}% 7
\BOOKMARK [2][-]{subsection.1.3.2}{Corpus}{section.1.3}% 8
\BOOKMARK [2][-]{subsection.1.3.3}{Training and Evaluation}{section.1.3}% 9
\BOOKMARK [2][-]{subsection.1.3.4}{Input Tokens}{section.1.3}% 10
\BOOKMARK [2][-]{subsection.1.3.5}{Encoder}{section.1.3}% 11
\BOOKMARK [2][-]{subsection.1.3.6}{Decoder}{section.1.3}% 12
\BOOKMARK [2][-]{subsection.1.3.7}{Output Tokens}{section.1.3}% 13
\BOOKMARK [2][-]{subsection.1.3.8}{Loss and Accuracy During Training}{section.1.3}% 14
\BOOKMARK [2][-]{subsection.1.3.9}{Attention Mechanism}{section.1.3}% 15
\BOOKMARK [2][-]{subsection.1.3.10}{Sequence to Sequence Chatbot}{section.1.3}% 16
\BOOKMARK [0][-]{chapter.2}{Transformers and The Generative Pre-training Transformer 2}{}% 17
\BOOKMARK [1][-]{section.2.1}{Transformer and Attention}{chapter.2}% 18
\BOOKMARK [2][-]{subsection.2.1.1}{Byte Pair Encoding}{section.2.1}% 19
\BOOKMARK [2][-]{subsection.2.1.2}{Attention}{section.2.1}% 20
\BOOKMARK [2][-]{subsection.2.1.3}{Encoder - Scaled Dot-Product Attention}{section.2.1}% 21
\BOOKMARK [2][-]{subsection.2.1.4}{Decoder Attention I - `Key' and `Value'}{section.2.1}% 22
\BOOKMARK [2][-]{subsection.2.1.5}{Decoder Attention II - `Query'}{section.2.1}% 23
\BOOKMARK [2][-]{subsection.2.1.6}{Decoder Attention II - Masking}{section.2.1}% 24
\BOOKMARK [2][-]{subsection.2.1.7}{Input - Positional Encoding}{section.2.1}% 25
\BOOKMARK [2][-]{subsection.2.1.8}{Output - Feed Forward Network}{section.2.1}% 26
\BOOKMARK [2][-]{subsection.2.1.9}{Visualization - Transformer}{section.2.1}% 27
\BOOKMARK [1][-]{section.2.2}{The Generative Pre-training Transformer 2 Model}{chapter.2}% 28
\BOOKMARK [2][-]{subsection.2.2.1}{Pre-Training}{section.2.2}% 29
\BOOKMARK [2][-]{subsection.2.2.2}{General}{section.2.2}% 30
\BOOKMARK [2][-]{subsection.2.2.3}{Corpus}{section.2.2}% 31
\BOOKMARK [2][-]{subsection.2.2.4}{Releases}{section.2.2}% 32
\BOOKMARK [2][-]{subsection.2.2.5}{Application Details}{section.2.2}% 33
\BOOKMARK [2][-]{subsection.2.2.6}{Visualization - GPT2}{section.2.2}% 34
\BOOKMARK [0][-]{chapter.3}{Experimental Design and Setup}{}% 35
\BOOKMARK [1][-]{section.3.1}{Approach to the Study}{chapter.3}% 36
\BOOKMARK [1][-]{section.3.2}{Hardware Overview}{chapter.3}% 37
\BOOKMARK [1][-]{section.3.3}{Setup}{chapter.3}% 38
\BOOKMARK [2][-]{subsection.3.3.1}{Graphical Processing Unit vs. Central Processing Unit}{section.3.3}% 39
\BOOKMARK [2][-]{subsection.3.3.2}{Raspberry Pi}{section.3.3}% 40
\BOOKMARK [2][-]{subsection.3.3.3}{Jetson Nano}{section.3.3}% 41
\BOOKMARK [2][-]{subsection.3.3.4}{Tensorflow vs. Pytorch}{section.3.3}% 42
\BOOKMARK [2][-]{subsection.3.3.5}{Speech and Speech To Text}{section.3.3}% 43
\BOOKMARK [2][-]{subsection.3.3.6}{Corpus Considerations}{section.3.3}% 44
\BOOKMARK [1][-]{section.3.4}{ARMv7 Build/Compile}{chapter.3}% 45
\BOOKMARK [2][-]{subsection.3.4.1}{Pytorch `torch' Library 1.1.0 For ARMv7}{section.3.4}% 46
\BOOKMARK [2][-]{subsection.3.4.2}{Pytorch `torch' Library 1.4.0 For ARMv7}{section.3.4}% 47
\BOOKMARK [2][-]{subsection.3.4.3}{Docker Container `tensorflow-model-server' For ARMv7}{section.3.4}% 48
\BOOKMARK [0][-]{chapter.4}{Experimental Results - Raspberry Pi}{}% 49
\BOOKMARK [1][-]{section.4.1}{Experiments - Installations}{chapter.4}% 50
\BOOKMARK [2][-]{subsection.4.1.1}{Questions}{section.4.1}% 51
\BOOKMARK [2][-]{subsection.4.1.2}{Checklist}{section.4.1}% 52
\BOOKMARK [1][-]{section.4.2}{Chatbot - Gated Recurrent Unit Model}{chapter.4}% 53
\BOOKMARK [2][-]{subsection.4.2.1}{Questions}{section.4.2}% 54
\BOOKMARK [2][-]{subsection.4.2.2}{Checklist}{section.4.2}% 55
\BOOKMARK [1][-]{section.4.3}{Smart Speaker - Gated Recurrent Unit Model}{chapter.4}% 56
\BOOKMARK [1][-]{section.4.4}{Chatbot - Transformer Model with Persona Corpus}{chapter.4}% 57
\BOOKMARK [2][-]{subsection.4.4.1}{Training}{section.4.4}% 58
\BOOKMARK [2][-]{subsection.4.4.2}{Questions}{section.4.4}% 59
\BOOKMARK [2][-]{subsection.4.4.3}{Checklist}{section.4.4}% 60
\BOOKMARK [1][-]{section.4.5}{Smart Speaker - Transformer Model with Persona Corpus}{chapter.4}% 61
\BOOKMARK [1][-]{section.4.6}{Chatbot - Transformer Model with Movie Corpus}{chapter.4}% 62
\BOOKMARK [2][-]{subsection.4.6.1}{Questions}{section.4.6}% 63
\BOOKMARK [2][-]{subsection.4.6.2}{Checklist}{section.4.6}% 64
\BOOKMARK [1][-]{section.4.7}{Smart Speaker - Transformer Model with Movie Corpus}{chapter.4}% 65
\BOOKMARK [1][-]{section.4.8}{Chatbot - Generative Pre-training Transformer 2 Model}{chapter.4}% 66
\BOOKMARK [2][-]{subsection.4.8.1}{Context Experiment}{section.4.8}% 67
\BOOKMARK [2][-]{subsection.4.8.2}{History Experiment}{section.4.8}% 68
\BOOKMARK [2][-]{subsection.4.8.3}{Artificial Intelligence Markup Language Experiment}{section.4.8}% 69
\BOOKMARK [2][-]{subsection.4.8.4}{Program Launching}{section.4.8}% 70
\BOOKMARK [2][-]{subsection.4.8.5}{Overall}{section.4.8}% 71
\BOOKMARK [2][-]{subsection.4.8.6}{Questions}{section.4.8}% 72
\BOOKMARK [2][-]{subsection.4.8.7}{Checklist}{section.4.8}% 73
\BOOKMARK [1][-]{section.4.9}{Smart Speaker - Generative Pre-training Transformer 2 Model}{chapter.4}% 74
\BOOKMARK [0][-]{chapter.5}{Further Installations}{}% 75
\BOOKMARK [1][-]{section.5.1}{Generative Pre-training Transformer 2 - XLarge Model}{chapter.5}% 76
\BOOKMARK [2][-]{subsection.5.1.1}{Context Experiment}{section.5.1}% 77
\BOOKMARK [2][-]{subsection.5.1.2}{History Experiment}{section.5.1}% 78
\BOOKMARK [2][-]{subsection.5.1.3}{Artificial Intelligence Markup Language Experiment}{section.5.1}% 79
\BOOKMARK [2][-]{subsection.5.1.4}{User Name Experiment}{section.5.1}% 80
\BOOKMARK [2][-]{subsection.5.1.5}{Internet Search Experiment}{section.5.1}% 81
\BOOKMARK [1][-]{section.5.2}{Generative Pre-training Transformer 2 \205 Jetson Nano}{chapter.5}% 82
\BOOKMARK [0][-]{chapter.6}{Observations and Conclusions}{}% 83
\BOOKMARK [1][-]{section.6.1}{GRU vs. Transformer}{chapter.6}% 84
\BOOKMARK [1][-]{section.6.2}{Turing Test}{chapter.6}% 85
\BOOKMARK [1][-]{section.6.3}{Word and Sentence Comparisons}{chapter.6}% 86
\BOOKMARK [2][-]{subsection.6.3.1}{Word Usage}{section.6.3}% 87
\BOOKMARK [2][-]{subsection.6.3.2}{Sentence Usage}{section.6.3}% 88
\BOOKMARK [1][-]{section.6.4}{Training and Lists}{chapter.6}% 89
\BOOKMARK [2][-]{subsection.6.4.1}{Transformer and GRU - Training Progress}{section.6.4}% 90
\BOOKMARK [2][-]{subsection.6.4.2}{Transformer and GRU - Learning Lists}{section.6.4}% 91
\BOOKMARK [0][-]{chapter.7}{Further Reading}{}% 92
\BOOKMARK [1][-]{section.7.1}{Winograd Schema}{chapter.7}% 93
\BOOKMARK [0][-]{appendix.A}{Abbreviations}{}% 94
