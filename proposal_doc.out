\BOOKMARK [0][-]{chapter.1}{Background/History of the Study}{}% 1
\BOOKMARK [1][-]{section.1.1}{Background}{chapter.1}% 2
\BOOKMARK [1][-]{section.1.2}{Recurrent Neural Network Components}{chapter.1}% 3
\BOOKMARK [2][-]{subsection.1.2.1}{Overview}{section.1.2}% 4
\BOOKMARK [2][-]{subsection.1.2.2}{Gated Recurrent Unit}{section.1.2}% 5
\BOOKMARK [1][-]{section.1.3}{Sequence to Sequence}{chapter.1}% 6
\BOOKMARK [2][-]{subsection.1.3.1}{Word Embeddings}{section.1.3}% 7
\BOOKMARK [2][-]{subsection.1.3.2}{Corpus}{section.1.3}% 8
\BOOKMARK [2][-]{subsection.1.3.3}{Input Tokens}{section.1.3}% 9
\BOOKMARK [2][-]{subsection.1.3.4}{Encoder}{section.1.3}% 10
\BOOKMARK [2][-]{subsection.1.3.5}{Decoder}{section.1.3}% 11
\BOOKMARK [2][-]{subsection.1.3.6}{Output Tokens}{section.1.3}% 12
\BOOKMARK [2][-]{subsection.1.3.7}{Loss and Accuracy During Training}{section.1.3}% 13
\BOOKMARK [2][-]{subsection.1.3.8}{Attention Mechanism}{section.1.3}% 14
\BOOKMARK [2][-]{subsection.1.3.9}{Sequence to Sequence Chatbot}{section.1.3}% 15
\BOOKMARK [0][-]{chapter.2}{Transformers and The Generative Pre-training Transformer 2}{}% 16
\BOOKMARK [1][-]{section.2.1}{Transformer and Attention}{chapter.2}% 17
\BOOKMARK [2][-]{subsection.2.1.1}{Byte Pair Encoding}{section.2.1}% 18
\BOOKMARK [2][-]{subsection.2.1.2}{Attention}{section.2.1}% 19
\BOOKMARK [2][-]{subsection.2.1.3}{Scaled Dot-Product Attention}{section.2.1}% 20
\BOOKMARK [2][-]{subsection.2.1.4}{Decoder}{section.2.1}% 21
\BOOKMARK [2][-]{subsection.2.1.5}{Input - Positional Encoding}{section.2.1}% 22
\BOOKMARK [2][-]{subsection.2.1.6}{Output - Feed Forward Network}{section.2.1}% 23
\BOOKMARK [2][-]{subsection.2.1.7}{Visualization - Transformer}{section.2.1}% 24
\BOOKMARK [1][-]{section.2.2}{The Generative Pre-training Transformer 2 Model}{chapter.2}% 25
\BOOKMARK [2][-]{subsection.2.2.1}{Pre-Training}{section.2.2}% 26
\BOOKMARK [2][-]{subsection.2.2.2}{General}{section.2.2}% 27
\BOOKMARK [2][-]{subsection.2.2.3}{Corpus}{section.2.2}% 28
\BOOKMARK [2][-]{subsection.2.2.4}{Releases}{section.2.2}% 29
\BOOKMARK [2][-]{subsection.2.2.5}{Application Details}{section.2.2}% 30
\BOOKMARK [2][-]{subsection.2.2.6}{Visualization - GPT2}{section.2.2}% 31
\BOOKMARK [0][-]{chapter.3}{Experiments}{}% 32
\BOOKMARK [1][-]{section.3.1}{Approach to the Study}{chapter.3}% 33
\BOOKMARK [1][-]{section.3.2}{Model Overview}{chapter.3}% 34
\BOOKMARK [1][-]{section.3.3}{Setup}{chapter.3}% 35
\BOOKMARK [2][-]{subsection.3.3.1}{Graphical Processing Unit vs. Central Processing Unit}{section.3.3}% 36
\BOOKMARK [2][-]{subsection.3.3.2}{Raspberry Pi}{section.3.3}% 37
\BOOKMARK [2][-]{subsection.3.3.3}{Tensorflow vs. Pytorch}{section.3.3}% 38
\BOOKMARK [2][-]{subsection.3.3.4}{Speech and Speech To Text}{section.3.3}% 39
\BOOKMARK [2][-]{subsection.3.3.5}{Corpus Considerations}{section.3.3}% 40
\BOOKMARK [1][-]{section.3.4}{ARMv7 Build/Compile}{chapter.3}% 41
\BOOKMARK [2][-]{subsection.3.4.1}{Pytorch `torch' Library 1.1.0 For ARMv7}{section.3.4}% 42
\BOOKMARK [2][-]{subsection.3.4.2}{Pytorch `torch' Library 1.4.0 For ARMv7}{section.3.4}% 43
\BOOKMARK [2][-]{subsection.3.4.3}{Docker Container `tensorflow-model-server' For ARMv7}{section.3.4}% 44
\BOOKMARK [1][-]{section.3.5}{Experiments - Installations}{chapter.3}% 45
\BOOKMARK [2][-]{subsection.3.5.1}{Chatbot - Gated Recurrent Unit Model}{section.3.5}% 46
\BOOKMARK [2][-]{subsection.3.5.2}{Smart Speaker - Gated Recurrent Unit Model}{section.3.5}% 47
\BOOKMARK [2][-]{subsection.3.5.3}{Chatbot - Transformer Model with Persona Corpus}{section.3.5}% 48
\BOOKMARK [2][-]{subsection.3.5.4}{Smart Speaker - Transformer Model with Persona Corpus}{section.3.5}% 49
\BOOKMARK [2][-]{subsection.3.5.5}{Chatbot - Transformer Model with Movie Corpus}{section.3.5}% 50
\BOOKMARK [2][-]{subsection.3.5.6}{Smart Speaker - Transformer Model with Movie Corpus}{section.3.5}% 51
\BOOKMARK [2][-]{subsection.3.5.7}{Chatbot - Generative Pre-training Transformer 2 Model}{section.3.5}% 52
\BOOKMARK [2][-]{subsection.3.5.8}{Smart Speaker - Generative Pre-training Transformer 2 Model}{section.3.5}% 53
\BOOKMARK [1][-]{section.3.6}{Observation}{chapter.3}% 54
\BOOKMARK [2][-]{subsection.3.6.1}{GRU vs. Transformer}{section.3.6}% 55
\BOOKMARK [2][-]{subsection.3.6.2}{Smaller Chatbot Learning}{section.3.6}% 56
\BOOKMARK [2][-]{subsection.3.6.3}{Word Usage}{section.3.6}% 57
\BOOKMARK [2][-]{subsection.3.6.4}{Sentence Usage}{section.3.6}% 58
\BOOKMARK [1][-]{section.3.7}{Tests}{chapter.3}% 59
\BOOKMARK [2][-]{subsection.3.7.1}{Turing Test}{section.3.7}% 60
\BOOKMARK [2][-]{subsection.3.7.2}{Winograd Schema}{section.3.7}% 61
\BOOKMARK [0][-]{appendix.A}{Further Installations}{}% 62
\BOOKMARK [1][-]{section.A.1}{Generative Pre-training Transformer 2 - Large Model}{appendix.A}% 63
\BOOKMARK [2][-]{subsection.A.1.1}{Context Experiment}{section.A.1}% 64
\BOOKMARK [2][-]{subsection.A.1.2}{History Experiment}{section.A.1}% 65
\BOOKMARK [2][-]{subsection.A.1.3}{Artificial Intelligence Markup Language Experiment}{section.A.1}% 66
\BOOKMARK [0][-]{appendix.B}{Abbreviations}{}% 67
