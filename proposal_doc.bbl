\begin{thebibliography}{10}

\bibitem{DBLP:journals/corr/VinyalsL15}
O.~Vinyals and Q.~V. Le, ``A neural conversational model,'' {\em CoRR},
  vol.~abs/1506.05869, 2015.

\bibitem{2018Inkawhich}
{Matthew Inkawhich}, ``pytorch-chatbot,'' 2018.
\newblock https://github.com/pytorch/tutorials
  /blob/master/beginner{\_}source/chatbot{\_}tutorial.py.

\bibitem{Vaswani2017AttentionIA}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  L.~Kaiser, and I.~Polosukhin, ``Attention is all you need,'' in {\em NIPS},
  2017.

\bibitem{radford2019language}
A.~Radford, J.~Wu, R.~Child, D.~Luan, D.~Amodei, and I.~Sutskever, ``Language
  models are unsupervised multitask learners,'' 2019.

\bibitem{Wolf2019HuggingFacesTS}
T.~Wolf, L.~Debut, V.~Sanh, J.~Chaumond, C.~Delangue, A.~Moi, P.~Cistac,
  T.~Rault, R.~Louf, M.~Funtowicz, and J.~Brew, ``Huggingface's transformers:
  State-of-the-art natural language processing,'' {\em ArXiv},
  vol.~abs/1910.03771, 2019.

\bibitem{2015Britz}
{Denny Britz}, ``Recurrent neural network tutorial, part 4 – implementing a
  gru/lstm rnn with python and theano,'' 2015.
\newblock
  http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/.

\bibitem{mikolov2013efficient}
T.~Mikolov, K.~Chen, G.~Corrado, and J.~Dean, ``Efficient estimation of word
  representations in vector space,'' {\em arXiv preprint arXiv:1301.3781},
  2013.

\bibitem{pennington-etal-2014-glove}
J.~Pennington, R.~Socher, and C.~Manning, ``{G}love: Global vectors for word
  representation,'' in {\em Proceedings of the 2014 Conference on Empirical
  Methods in Natural Language Processing ({EMNLP})}, (Doha, Qatar),
  pp.~1532--1543, Association for Computational Linguistics, Oct. 2014.

\bibitem{DBLP:journals/corr/LuongPM15}
M.~Luong, H.~Pham, and C.~D. Manning, ``Effective approaches to attention-based
  neural machine translation,'' {\em CoRR}, vol.~abs/1508.04025, 2015.

\bibitem{radford2018improving}
A.~Radford, K.~Narasimhan, T.~Salimans, and I.~Sutskever, ``Improving language
  understanding by generative pre-training,'' {\em URL https://s3-us-west-2.
  amazonaws. com/openai-assets/researchcovers/languageunsupervised/language
  understanding paper. pdf}, 2018.

\bibitem{2019NVIDIAadlr}
{NVIDIA Applied Deep Learning Research}, ``Megatronlm: Training billion+
  parameter language models using gpu model parallelism,'' 2019.
\newblock https://nv-adlr.github.io/MegatronLM.

\bibitem{DBLP:journals/corr/abs-1809-01984}
P.~Mazar{\'{e}}, S.~Humeau, M.~Raison, and A.~Bordes, ``Training millions of
  personalized dialogue agents,'' {\em CoRR}, vol.~abs/1809.01984, 2018.

\bibitem{2018Milosevic}
{Nemanja Milosevic}, ``Compling arm stuff without an arm board / build pytorch
  for the raspberry pi,'' 2019.
\newblock
  https://nmilosev.svbtle.com/compling-arm-stuff-without-an-arm-board-build-pytorch-for-the-raspberry-pi.

\bibitem{2020Maciejewski}
{Erik Maciejewski}, ``Tensorflow serving arm - a project for cross-compiling
  tensorflow serving targeting popular arm cores,'' 2020.
\newblock https://github.com/emacski/tensorflow-serving-arm.

\bibitem{youtubeKaiser2017}
Łukasz Kaiser, ``Attention is all you need; attentional neural network models
  | Łukasz kaiser | masterclass,'' 2017.
\newblock https://youtu.be/rBCqOTEfxvg.

\bibitem{wiki:xxx}
{Wikipedia contributors}, ``Winograd schema challenge --- {Wikipedia}{,} the
  free encyclopedia.''
  \url{https://en.wikipedia.org/w/index.php?title=Winograd_Schema_Challenge&oldid=938834078},
  2020.
\newblock [Online; accessed 15-February-2020].

\bibitem{brown2020language}
T.~B. Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~Kaplan, P.~Dhariwal,
  A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, S.~Agarwal, A.~Herbert-Voss,
  G.~Krueger, T.~Henighan, R.~Child, A.~Ramesh, D.~M. Ziegler, J.~Wu,
  C.~Winter, C.~Hesse, M.~Chen, E.~Sigler, M.~Litwin, S.~Gray, B.~Chess,
  J.~Clark, C.~Berner, S.~McCandlish, A.~Radford, I.~Sutskever, and D.~Amodei,
  ``Language models are few-shot learners,'' 2020.

\end{thebibliography}
