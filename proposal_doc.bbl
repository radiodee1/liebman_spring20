\begin{thebibliography}{10}

\bibitem{DBLP:journals/corr/VinyalsL15}
O.~Vinyals and Q.~V. Le, ``A neural conversational model,'' {\em CoRR},
  vol.~abs/1506.05869, 2015.

\bibitem{2018Inkawhich}
{Matthew Inkawhich}, ``pytorch-chatbot,'' 2018.
\newblock https://github.com/pytorch/tutorials
  /blob/master/beginner{\_}source/chatbot{\_}tutorial.py.

\bibitem{Vaswani2017AttentionIA}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  L.~Kaiser, and I.~Polosukhin, ``Attention is all you need,'' in {\em NIPS},
  2017.

\bibitem{radford2019language}
A.~Radford, J.~Wu, R.~Child, D.~Luan, D.~Amodei, and I.~Sutskever, ``Language
  models are unsupervised multitask learners,'' 2019.

\bibitem{Wolf2019HuggingFacesTS}
T.~Wolf, L.~Debut, V.~Sanh, J.~Chaumond, C.~Delangue, A.~Moi, P.~Cistac,
  T.~Rault, R.~Louf, M.~Funtowicz, and J.~Brew, ``Huggingface's transformers:
  State-of-the-art natural language processing,'' {\em ArXiv},
  vol.~abs/1910.03771, 2019.

\bibitem{2015Britz}
{Denny Britz}, ``Recurrent neural network tutorial, part 4 â€“ implementing a
  gru/lstm rnn with python and theano,'' 2015.
\newblock
  http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/.

\bibitem{DBLP:journals/corr/LuongPM15}
M.~Luong, H.~Pham, and C.~D. Manning, ``Effective approaches to attention-based
  neural machine translation,'' {\em CoRR}, vol.~abs/1508.04025, 2015.

\bibitem{Danescu-Niculescu-Mizil+Lee:11a}
C.~Danescu-Niculescu-Mizil and L.~Lee, ``Chameleons in imagined conversations:
  A new approach to understanding coordination of linguistic style in
  dialogs.,'' in {\em Proceedings of the Workshop on Cognitive Modeling and
  Computational Linguistics, ACL 2011}, 2011.

\bibitem{DBLP:journals/corr/abs-1809-01984}
P.~Mazar{\'{e}}, S.~Humeau, M.~Raison, and A.~Bordes, ``Training millions of
  personalized dialogue agents,'' {\em CoRR}, vol.~abs/1809.01984, 2018.

\bibitem{mikolov2013efficient}
T.~Mikolov, K.~Chen, G.~Corrado, and J.~Dean, ``Efficient estimation of word
  representations in vector space,'' {\em arXiv preprint arXiv:1301.3781},
  2013.

\bibitem{pennington-etal-2014-glove}
J.~Pennington, R.~Socher, and C.~Manning, ``{G}love: Global vectors for word
  representation,'' in {\em Proceedings of the 2014 Conference on Empirical
  Methods in Natural Language Processing ({EMNLP})}, (Doha, Qatar),
  pp.~1532--1543, Association for Computational Linguistics, Oct. 2014.

\bibitem{tensor2tensor}
A.~Vaswani, S.~Bengio, E.~Brevdo, F.~Chollet, A.~N. Gomez, S.~Gouws, L.~Jones,
  L.~Kaiser, N.~Kalchbrenner, N.~Parmar, R.~Sepassi, N.~Shazeer, and
  J.~Uszkoreit, ``Tensor2tensor for neural machine translation,'' {\em CoRR},
  vol.~abs/1803.07416, 2018.

\bibitem{DBLP:journals/corr/abs-1810-04805}
J.~Devlin, M.~Chang, K.~Lee, and K.~Toutanova, ``{BERT:} pre-training of deep
  bidirectional transformers for language understanding,'' {\em CoRR},
  vol.~abs/1810.04805, 2018.

\end{thebibliography}
