\section{Approach to the Study}

Several tasks are necessary for the project. One task is to implement
the algorithms for different models, one the sequence to sequence model
and the other the transformer model for a generative chatbot and finally the GPT2 model.

We will not try to rewrite the transformer or GPT2 model ourselves.

In this project we attempt to load as much of out chatbot code onto a Raspberry Pi as possible. We have trained models using the pytorch and tensorflow libraries. These models are responsible for taking in english sentences and producing english output. There is another part of the typical Raspberry Pi setup that includes another neural network component. Speech to text models, which our application requires, rely on large neural network resources. For this purpose we use speech to text resources supplied by Google on the google cloud. To include speech to text libraries locally on the Raspberry Pi would be too costly in computation time and resources like RAM. It would also be complicated to implement technically. It could easily comprise an entire project on its own.

Unfortunately the speech to text resources supplied by Google cost money. To use the service you need to have a billing account with Google.

The speech to text service used on the project and the memory limitations on the Raspberry Pi leads one to ask the question weather the neural network responsible for the chatbot function could not be servable from some faster machine located somewhere on the internet. At this time we are not interested in serving these resources. It would entail two calls from the Raspberry Pi for every sentence. This complicates things and also has a time overhead. 

Also, we have several models that we want to test. To test them all would require several servers. Also we use both Pytorch and tensorflow. Tensorflow has `tensorflow-model-server' for serving models, but Pytorch has no equivalent.

It is important to note that the large GPT2 model specifically could be served from a remote computer and it would operate faster. Currently on the Raspberry Pi decoding a single sentence takes approximately 13 seconds. Even so, we prefer to install our trained models on the Raspberry Pi directly.

\section{Setup}

We use linux computers, sometimes with gpu hardware for parallel processing.
We also use the Python programming language. Code from this project
can be run with the 3.x version of Python.

When the project was started we did some programming with Keras using
Tensorflow as a backend. Keras was later discarded in favor of Pytorch.
Pytorch as a library is still under development at the time of this
writing.

Some of the GPT2 code uses Pytorch. Some of the Transformer and GPT2 code uses Tensorflow. There is a repository on Github that has the GPT2 trained model using Pytorch instead of Tensorflow.

We use github as a code repository. Code corresponding with this paper
can be found at: \href{https://github.com/radiodee1/awesome-chatbot}{https://github.com/radiodee1/awesome-chatbot}
. 

As a coding experiment we rewrite the code for the sequence-to-sequence GRU model. We have varying amounts of success with these experiments. We do not rewrite the GPT2 code from the Tensorflow or Pytorch repository.

\section{Speech and Speech To Text}

Google has python packages that translate text to speech and speech to text. In the case of text to speech the library is called `gTTS'. In the case of speech to text the library is called `google-cloud-speech'. 

The gTTS package is simple to use and can be run locally without connection to the internet. The google-cloud-speech package uses a google cloud server to take input from the microphone and return text. For this reason it requires an internet connection and an account with Google that enables google cloud api use. Google charges the user a small amount for every word that they translate into text. 

Both of these resources, the text-to-speech and speech-to-text, work out of the box on the Raspberry Pi, but configuring speech-to-text for the Pi is not trivial. The user must register a billing account with Google cloud services. In return for this registration the user is allowed to download a json authentication file. The file must be copied to the Raspberry Pi. 

Furthermore an environment variable must be set that points to the authentication file. The variable is called `GOOGLE\_APPLICATION\_CREDENTIALS'. This environment variable has to be set up before the respective model runs. When the model is launched on startup it may not be launched as a regular user. The model may be launched as, for example, the root user. Somehow the environment variable must be set along with the launching of the neural network model.

The operating system on the Raspberry Pi is based on Debian Linux. In this operating system there is a file which is run immediately after the basic system starts up. This script is called `\textbf{/etc/rc.local}'. It is sufficient to put the environment variable there and follow it with the launching of the model. To ensure that the process goes without a hitch, we attempt to combine the setting of the environment variable with the launching of the program in a single line of code.

\section{ARMv7 Build/Compile}

\subsection*{Pytorch `torch' Library 1.1.0 For ARMv7}
We compile the Pytorch library for Raspberry Pi. We use several virtualization techniques to do this compilation. The result of those efforts is a Pytorch python 3.7 library for the Raspberry Pi.

On their web site Milosevic et al (2019)\cite{2018Milosevic} compile pytorch 1.1.0 for the Raspberry Pi. We follow their instructions closely. We are able to build the package for the ARMv7 platform.

The instructions called for constructing a change-root environment where a Fedora Core 30 linux system was set up. Then the ARMv7 system was used in the change-root environment to compile the Pytorch library for the 1.1.0 version.

The production laptop used for development ran Ubuntu linux. For this reason a Virtualbox emulation was set up with Fedora Core 30 on it. Inside that emulator the change-root environment was set up. The library was compiled there successfully. 

There are two problems with the resulting built python package. Firstly there is an error in python when importing the torch library. The error reads `ImportError: No module named \_C'. 

After some research it is clear that the build process for ARMv7 creates some shared object files that are misnamed. A hackey fix is to find the misnamed files and make copies of them with a suitable name. The same outcome could be assured by making symbolic links to the misnamed files with proper names.

There are three files misnamed. They can be found at `\textbf{/usr/lib/python3.7/site-packages/torch/}'. They are named with the same convention. They all have the ending `.cpython-37m-arm7hf-linux-gnu.so'. We want to rename them with the much shorter `.so'. The files are then named `\textbf{\_C.so}', `\textbf{\_dl.so}', and `\textbf{\_thnn.so}'.

This takes care of the `ImportError'. The second problem is that the version of GLIBC in the change-root environment does not match the GLIBC library in the Raspberry Pi Raspbian distribution. This produces the following error: `\textbf{ImportError: /usr/lib/x86\_64-linux-gnu/libstdc++.so.6: version `GLIBCXX\_3.4.26' not found}'.

This is solved by rebuilding the package with Fedora Core 29 instead of 30. 
 
\subsection*{Pytorch `torch' Library 1.4.0 For ARMv7}
We recompile the Pytorch library for the Raspberry Pi. We use debian virtualization techniques for the compilation. Because ubuntu is a debian derivative it is not necessary to run the process in a Virtualbox container. 

In addition to this, the files created by the compilation are properly named. There is no need to go to the directory `\textbf{/usr/lib/python3.7/site-packages/torch/}' to change anything. 

The time spent compiling the software is approximately 5 hours. Time spent with the Virtualbox container was easily twice that. The time spent on the Raspberry Pi executing a single GPT2 question and answer remains about 13 seconds, so there was no gain in that respect.

There were several small hurdles to completing the compilation. Firstly the `debootstrap' command needed to be employed at the start. Debian Stretch was used as the host operating system. It was felt that if it was used that the GLIBC compatibility problem would not be faced. This turned out to be the case.

There are some dependencies that need to be installed on the `chroot' environment for Pytorch to compile. One of these is that is important is `libblas3.'

Then Python 3.7 needed to be built on Stretch. The Stretch program repositories use Python 2.7 and 3.5 . The Rapbian operating system on the Raspberry Pi 4B is based on Debian Buster. After compiling Python 3.7 the Git program needed to be compiled from scratch. Git on Stretch has a issue that is fixed upstream, but we want to use Stretch because of the GLIBC issue.

It is conceivable that the GLIBC issue would not be important if the `chroot' environment used Debian Buster, since that is the basis for the Raspbian operating system. The Stretch operating system solution works though.

Finally the Pytorch program needed to be built. We disable CUDA and distributed computing as neither exists on the Raspberry Pi.

\subsection*{Docker Container `tensorflow-model-server' For ARMv7}
The Google machine learning library for python uses a standalone program called `tensorflow-model-server' for serving all tensorflow models in a standard way. The program has not been officially compiled for ARMv7. There exists, though, a docker image that will run on ARMv7.

The original idea was to follow someone else's instructions and compile the Docker Container for the ARMv7. Then the executable would be removed from the container and used natively in the Raspberry Pi.

It was found that there existed a version of the Docker Container Daemon that ran on the Raspberry Pi. All that remained was to write a Docker Container script that interacted with the existing ARMv7 container. The author of the original container is Erik Maciejewski (2020)\cite{2020Maciejewski}.



\section{Experiments}
In the section above we describe the workings of a transformer and the workings of GPT2. We propose they are similar. Here we distinguish between the two. For the experiments section they are totally separate.

We have several basic neural network models. One is the basic sequence to sequence model typically used for neural machine translation. We also have the transformer and the GPT2. We try to touch on each model type and we also distinguish between chatbot operation and smart-speaker operation. This gives us six sections.

The GPT2 code is versatile. We use it for the chatbot problem. The GPT2 model is pretrained. We experiment with transfer learning and further training of the GPT2 model, but in our case it does not improve the model's performance. 

When we talk about the chatbot problem on GPT2 we are talking about a PyTorch version of the GPT2 code. 

We found that the chatbot with the hand coded sequence to sequence NMT did not work very well. There is a tensorflow transformer-based model that worked marginally well. We found that the GPT2 chatbot worked well enough with the `zero-shot' setup so that fine-tuning was not necessary. 

Fine tuning on the GPT2 problem actually had a negative effect. To fine-tune GPT2 also involved training the model on Tensorflow and then translating the model to PyTorch when that was done. 

We use speech recognition and speech to text libraries to allow a user to give the chatbot model auditory input.

We also train the transformer to do the chatbot task. This works better than the the hand coded sequence to sequence model but not as well as the GPT2. 

For the sake of experimentation we use a sequence to sequence GRU tutorial and implement the chatbot. This model works well and allows us to compare the GRU model with the GPT2 model.

Finally we describe installing the chatbot in small computing platforms, the Raspberry Pi 3B and 4B, in an attempt to create a smart speaker.

All of the three mentioned models work on a laptop computer. For the laptop we experimented with a setup where the model would launch applications for the user when the user asked the model to. These audio cues were interesting but were in no way the focus of our experiments.

It should be noted that at some point we would like to describe some of our subjective results with the different models. It is difficult to measure our results objectively because we are using code most closely associated with a translation task. 

If we were measuring progress with an actual translation task accuracy could be measured as a score that improves when the output matches the meaning of the input, albeit in a different language. We don't use an actual translation task, we use something close to one. The problem is that though the input and the output are in the same language, the input and output have different meanings. We do not dictate what the correct output would be for a given input. The output just has to make sense as a reply in the English language. We would actually prefer that the output not have the same meaning as the input. This makes it difficult to calculate an objective accuracy score. This is touched on in the paper from Vinyals et al(2015)\cite{DBLP:journals/corr/VinyalsL15}.

Loss can still be monitored during training. When the loss stops decreasing you know you should stop training as the model is probably overfitting.

\subsubsection*{Questions}
Below is a list of questions asked of all models.

%\begin{lstlisting}[language=bash]

\texttt{Hello.}

\texttt{What is your name?}

\texttt{What time is it?}

\texttt{What do you do?}

\texttt{What is your favorite color?}

\texttt{Do you like red?}

\texttt{Do you like blue?}

\texttt{What is your favorite candy?}

\texttt{Do you like ice cream?}

\texttt{Good bye.}

%\end{lstlisting}

\subsection{Chatbot - GRU Model}
We have two models based on the sequnce to sequence architecture proposed by Vinyals et al (2015)\cite{DBLP:journals/corr/VinyalsL15}. One model was largely written by hand. This first model does not work very well. The second model was taken largely from an online tutorial by Matthew Inkawhich (2018)\cite{2018Inkawhich}.

We trained the first sequence to sequence model on a large english
corpus in an attempt to produce a chatbot. 

For this sequence to sequence model we want to use text found in a movie dialog corpus (Danescu-Niculescu-Mizil et al, 2011)\cite{Danescu-Niculescu-Mizil+Lee:11a}. 

In our experience with coding our own chatbot we find that the model learns a single English sentence that can be used for most replies. 

Subjectively the GRU chatbot is unusable.

For example our chatbot frequently replies to input with the phrase `I don't know'. This is the most common output from our model. In this respect our original chatbot responds poorly.

A large portion of the time the bot's response to a question is the same. This is interesting in that the chatbot has identified that this phrase is a suitable answer to many questions, but it is disappointing that there is not more variety to the output.  

Occasionally the sequence-to-sequence model will reply with the phrase "I'm sorry." This happens very infrequently and it is not clear why the model chooses to reply this way. 

The second sequence to sequence model performed exactly as expected. It answered a number of subjective questions with original, authentic answers. The answers all made sense in the English language.

\subsubsection*{Questions}
This is the sample question list as it was answered by the second model.

\texttt{> hello .} 

\texttt{you re a good man .}

\texttt{> what is your name ?} 

\texttt{smith .}

\texttt{> what time is it ?} 

\texttt{i don t know .}

\texttt{> what do you do ?} 

\texttt{i m a comedian .}

\texttt{> what is your favorite color ?} 

\texttt{i don t know .}

\texttt{> do you like red ?} 

\texttt{no .}

\texttt{> do you like blue ?} 

\texttt{no .}

\texttt{> what is your favorite candy ?} 

\texttt{it s a big one .}

\texttt{> do you like ice cream ?} 

\texttt{no .}

\texttt{> good bye .} 

\texttt{good bye .}


\subsection{Smart Speaker - GRU Model}

The GRU model was installed on a Raspberry Pi. This allowed us to test out speech-to-text and text-to-speech libraries. The Raspberry Pi model was 3B. The RAM requirements were less than 500MB and the trained model answered questions on the Raspberry Pi almost instantaneously.

It also allowed us to compile the Pytorch library for Raspberry Pi.

The Raspberry Pi was outfitted with a microphone and a speaker and nothing more. It was also configured so that the Pytorch sequence to sequence model ran automatically on startup.

The model requires access to the internet for the exchange that the speech to text software has to make with the Google servers. If there is no internet the model doesn't work.

As there was no monitor and it took some time for the model to launch, the program was coded to beep when the model was ready to accept input.

\subsection{Chatbot - Transformer Model}
Using the Persona corpus we trained a transformer model to use as a chatbot. This transformer was not pre-trained with any large corpus, so this example did not use transfer learning. 

Subjectively this model did not perform as well as the GPT2 model, but it did perform better than the GRU model.

The memory footprint of the model while it was running was below 1 Gigabyte. It is conceivable that the model could be installed on a Raspberry Pi board but it requires a python package called `tensorflow-model-server' and this package would have to be built from source for the Raspberry Pi. 

Training of the model followed a certain pattern. First the model was trained on the persona corpus until a familiar pattern emerged. When the model began to answer all questions with the 
phrase "I don't know" training was stopped. 

At that time the corpus was modified to include no 
sentences that have the word "don't" in them. Training was started again until the output contained nothing but the phrase "I'm sorry." 

At that time the corpus was modified to include no sentences that have the word "sorry" in them.
Training was started again and was continued for some period. Training was stopped. A further segment of training was not attempted. 

At this point, after looking at the change in loss, further training was not
thought of as helpful. Loss stopped improving at some point in this process, and this lack of
improvement was taken as a sign that progress was not likely.

Subjectively the transformer model is better than the GRU model. It can respond to something like
four sentences. When it comes upon a question that it doesn't expect it defaults to a certain sentence. It can answer questions that you might ask in a rudimentary conversation. It has answers to prompts like `hi', `How are you?' and `What do you do?'. If you tell it your name it will tell you that its name is `Sarah'. It doesn't answer arbitrary questions. It cannot answer 'What is your favorite color?'. It cannot tell you the time. The default reply sentence for unknown prompts is `Hi, how are you today?'


%\begin{lstlisting}[language=bash]
\subsubsection*{Questions}
This is the sample question list as it was answered by the model.

\texttt{> hello}

\texttt{hi , how are you today ?}

 \texttt{> what is your name?}

\texttt{hi , how are you today ?}

\texttt{> what time is it ?}

\texttt{I like to read a lot}

 \texttt{> what do you do ?}

\texttt{i'm a student} 

 \texttt{> what is your favorite color ?}

\texttt{hi , how are you today ?}

 \texttt{> do you like red ?}

\texttt{hi , how are you today ?}

 \texttt{> do you like blue ?}

\texttt{hi , how are you today ?}

 \texttt{> what is your favorite candy ?}

\texttt{hi , how are you today ?}

 \texttt{> do you like ice cream ?}

\texttt{yes , i do }

 \texttt{> good bye}

\texttt{hi , how are you today ?}



%\end{lstlisting}

\subsection{Smart Speaker - Transformer Model}

Later we look to install the transformer model on the Raspberry Pi. This model is more dynamic than the sequence-to-sequence model. 

The transformer model takes about two minutes to boot on the Raspberry Pi. After that the time between responses is slow. The time between the first two or three responses is uncomfortably slow. After those first responses the time between answers gets to be more natural.



\subsection{Chatbot - GPT2 Model}
We used a pre-trained GPT2 model with the large english corpus to produce a chatbot and ascertain if this model works better than the sequence-to-sequence model. In our tests this worked well. The corpus is called `WebText'.

For our experiments GPT2 was used for the chatbot model in `zero-shot' mode. This means we did no
special fine-tuning of the GPT2 model in the application.

We did do some special coding for the input and output of the GPT2 code in order to operate it as
a chatbot. Input and output was limited to about 25 tokens. 

Input to the model was prepended with the character string "Q:" by our code. Output was observed 
to have the character string "A:" prepended to it. We assume therefore that the GPT2 model was at some point
exposed to the "Question/Answer" paradigm in written passages during its training. This was helpful.

Output from the GPT2 model was 
usually larger in size than we needed. Also, output had the character of having some sensible utterance followed by some output that was only a partial sentence.

It was necessary to `scrape' the output. First the output was checked for the "A:" character string at the start. If it was there it was removed. Then the first complete sentence was used as output, while words and phrases after that were discarded.

Lastly we decided that we would attempt to give the model some details that it could draw on 
during normal execution. We had two choices here. One choice was to train the model using fine-tuning and transfer learning to recognize certain questions and to supply answers. The other
choice was to simply show the model the list of facts that we thought were important before 
every input sequence. This information would be summarized with each reply.

The second choice was more interesting. The text that the model was shown always included the name of 
the model (picked somewhat arbitrarily) along with information about the location of the model
and the occupation. The time was also included.

Subjectively the model was the best of the three tested. The model would answer questions about it's location, it's name, and the time, faithfully most
of the time. Interestingly there where times when it did not do so. Some times it used 
alternative answers. For example, it would answer with the time but not the correct time. This was odd.

Under almost all circumstances the output was sensible English. There were few if any times where the model replied with gibberish. 

The subject matter of the prompts did not need to be the same as the simple introductory conversation of the transformer model. In fact any subject matter could be chosen and the model would answer. The model did not remember its own answers but it was consistent. Questions it answered include `What is your favorite color?' and `Do you like lollipops?'. 

\subsubsection*{Questions}
This is the sample question list as it was answered by the model. Note that the information mentioned in the answer about the time was accurate when the test was run.

\texttt{> hello}

\texttt{Hello.}

\texttt{> what is your name ?}

\texttt{My name is Jane.}

\texttt{> what time is it ?}

\texttt{02:59 PM January 28, 2020.}

\texttt{> what do you do ?}

\texttt{I am a student.} 

\texttt{> what is your favorite color ?}

\texttt{I love the color of the rainbow.}

\texttt{> do you like red ?}

\texttt{Yes.}

\texttt{> do you like blue ?}

\texttt{I do.}

\texttt{> what is your favorite candy ?}

\texttt{I love candy.}

\texttt{> do you like ice cream ?}

\texttt{I do. }

\texttt{> good bye}

\texttt{Good bye.}



\subsection{Smart Speaker - GPT2 Model}
Tests showed that the GPT2 chatbot worked well. We wanted to continue and allow the chatbot to
have more of the abilities of a smart speaker. We constructed a simple corpus that contained
key phrases that we wanted the chatbot to recognize and act upon. We did some transfer learning
with this new corpus.

We found that one of two things would happen. The chatbot would either learn the new phrases and forget all it's pre-training, or it would not learn the new phrases and it would retain all it's 
pre-training. For our examples there seemed to be no middle ground. Comparisons were made with 
all available models and a version without the transfer learning was settled on.

Code was added that uses Text To Speech and Speech To Text libraries. In this way the model could
interact with a subject using auditory cues and commands.

We did some programming that allowed the GPT2 model to launch programs when directed to by
the user. In this way we have tried to move our project closer to the smart-speakers that
are produced commercially. The programming did not rely on the neural-network aspects of the
model. Instead the code used heuristics and simple word recognition. This code can be disabled
when the model is run from the command line.


