\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\zref@newlabel[2]{}
\newlabel{chapter-transformer}{{2}{16}{Transformers and the Generative Pre-Training 2 Transformer}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Transformer and Attention}{16}{section.2.1}\protected@file@percent }
\newlabel{transformer-intro}{{2.1}{16}{Transformer and Attention}{section.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Byte Pair Encoding}{16}{subsection.2.1.1}\protected@file@percent }
\acro@used@once {BPE}{16}{16}{25}
\acro@used@once {OOV}{16}{16}{25}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Attention}{16}{subsection.2.1.2}\protected@file@percent }
\citation{Vaswani2017AttentionIA}
\citation{Vaswani2017AttentionIA}
\citation{Vaswani2017AttentionIA}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Transformer Encoder and Decoder}}{17}{figure.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Encoder - Scaled Dot-Product Attention}{18}{subsection.2.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Lowering Dimensionality}}{18}{figure.2.2}\protected@file@percent }
\newlabel{diagram-mat-mult-01}{{2.2}{18}{Lowering Dimensionality}{figure.2.2}{}}
\citation{Vaswani2017AttentionIA}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Attention Output}}{19}{figure.2.3}\protected@file@percent }
\newlabel{attantion-7}{{2.3}{19}{Attention Output}{figure.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Matching Input and Output}}{20}{figure.2.4}\protected@file@percent }
\newlabel{attention-matching}{{2.4}{20}{Matching Input and Output}{figure.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Transformer Encoder and Decoder Flow}}{21}{figure.2.5}\protected@file@percent }
\newlabel{diagram-flow1}{{2.5}{21}{Transformer Encoder and Decoder Flow}{figure.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Encoder Attention Detail}}{22}{figure.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Decoder Attention I - `Key' and `Value'}{22}{subsection.2.1.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Decoder Flow}}{23}{figure.2.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Decoder Attention Detail}}{24}{figure.2.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.5}Decoder Attention II - `Query'}{24}{subsection.2.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.6}Decoder Attention II - Masking}{24}{subsection.2.1.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Decoder Mask}}{25}{figure.2.9}\protected@file@percent }
\newlabel{diagram-mask-01}{{2.9}{25}{Decoder Mask}{figure.2.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.7}Input - Positional Encoding}{25}{subsection.2.1.7}\protected@file@percent }
\citation{Vaswani2017AttentionIA}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.8}Output - Feed Forward Network}{26}{subsection.2.1.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.9}Visualization - Transformer}{26}{subsection.2.1.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Visualized Attention Transformer}}{26}{figure.2.10}\protected@file@percent }
\citation{radford2018improving}
\citation{radford2018improving}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}The Generative Pre-Training 2 Model}{27}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Pre-Training}{27}{subsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}General}{27}{subsection.2.2.2}\protected@file@percent }
\acro@used@once {GPT2}{27}{27}{36}
\citation{Vaswani2017AttentionIA}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Generative Pre-Training 2 }}{28}{figure.2.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Training}{28}{subsection.2.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Inference}{29}{subsection.2.2.4}\protected@file@percent }
\citation{radford2019language}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces Generative Pre-Training 2 Inference}}{30}{figure.2.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}Corpus}{30}{subsection.2.2.5}\protected@file@percent }
\citation{2019NVIDIAadlr}
\citation{radford2019language}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.6}Releases}{31}{subsection.2.2.6}\protected@file@percent }
\@writefile{lot}{\contentsline {section}{GPT2 Size Overview}{31}{subsection.2.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.7}Application Details}{31}{subsection.2.2.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.8}Visualization - GPT2}{32}{subsection.2.2.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces Visualized Attention GPT2}}{32}{figure.2.13}\protected@file@percent }
\newlabel{diagram-vis04}{{2.13}{32}{Visualized Attention GPT2}{figure.2.13}{}}
\@setckpt{chapters/chapter02}{
\setcounter{page}{34}
\setcounter{equation}{0}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{2}
\setcounter{subsection}{8}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{13}
\setcounter{table}{0}
\setcounter{float@type}{8}
\setcounter{Item}{0}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{41}
\setcounter{parentequation}{0}
\setcounter{cp@cntr}{0}
\setcounter{lstnumber}{1}
\setcounter{section@level}{2}
\setcounter{lstlisting}{0}
}
