%\chapter{Background/History of the Study }


\section{Background}

In their paper Vinyals et al (2015)\cite{DBLP:journals/corr/VinyalsL15} discuss making a chatbot using
a neural network configured for sequence to sequence neural machine
translation. We code our own sequence to sequence chatbot, though our results are less than
spectacular. As our hand coded model does not run sufficiently well, we use code authored by Matthew Inkawhich (2018)\cite{2018Inkawhich}.

In their paper Vaswani et al (2017)\cite{Vaswani2017AttentionIA} discuss using the Transformer architecture for solving machine learning tasks. We train a transformer model as a chatbot.


Also Radford et al (2019)\cite{radford2019language} discuss the GPT2 neural network for NLP tasks. The GPT2 model is based largely on the Transformer architecture. GPT2 stands for 'Generative Pre-training Transformer 2'. 

We implement a chatbot with a GPT2 model. We use a program library from Wolf et al (2019)\cite{Wolf2019HuggingFacesTS} to run our model.


%\subsection{Background}

It is worth noting that with the appearance of the Transformer architecture and WordPiece vocabulary scheme, some technologies have become redundant or obsolete. This may be true of any model that uses RNN components and also the traditional word vector embeddings.
