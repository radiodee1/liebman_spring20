\chapter{Background/History of the Study}
\section{Background}
Transformer style models and also a Recurrent Neural Network model are used to allow for meaningful comparison, along with the Generative Pre-Training 2 Transformer. \ac{RNN} models are explained in Chapter \ref{chapter-recurrent} and Transformers in are explained in Chapter \ref{chapter-transformer}.

It is worth noting that with the appearance of the Transformer architecture some traditional technologies have become redundant or obsolete. This may be true of any model that uses Recurrent Neural Network components and also the traditional word vector embeddings.

\subsection{Recurrent Neural Network and Transformer}

In their paper Vinyals et al \cite{DBLP:journals/corr/VinyalsL15} discuss making a chatbot using a neural network configured for Sequence-to-Sequence Neural Machine Translation. An attempt to code our own Sequence-to-Sequence model was not very fruitful so instead the thesis uses code authored by Inkawhich et al \cite{2018Inkawhich}.

In their paper Vaswani et al \cite{Vaswani2017AttentionIA} discuss using the Transformer architecture for solving machine learning tasks. A transformer model is trained as a chatbot.

In both of these experiments a common factor is the Movie Dialog Corpus that the models train on. The corpus comes from Danescu-Niculescu-Mizil et al \cite{Danescu-Niculescu-Mizil+Lee:11a}.

\subsection{Pre Trained Language Models}
Radford et al \cite{radford2019language} discuss the ``Generative Pre-Training 2'' (GPT2) neural network for Natural Language Processing (\ac{NLP}) tasks. The GPT2 model is based largely on the Transformer architecture. 

This is essentially a Language Model. The model is given a large section of text during training and it is asked to generate a single word or token to add to the end of the sample. During inference it does this over and over to complete a passage. This kind of model shows up in GPT2 and the thesis uses that trained model and some traditional programming techniques to generate text that naturally answers questions. Among all the models tested, this is the model that was found to be most comprehensive in execution.

Several chatbots are implemented, one with a GPT2 model using a program library from Wolf et al \cite{Wolf2019HuggingFacesTS} to run the model.

\section{Outline}
This thesis starts with explanations of how some Recurrent Neural Network components work, followed by a discussion of Transformers. %Then the thesis discusses the GPT2 transformer.

The thesis goes on to describe installation of chatbot models on Raspberry Pi 4B computers, and on a Jetson Nano computer.

After that the thesis describes some further installations on an X86\_64 computer. Some graphs are given that show word and sentence usage of the Gated Recurrent Unit and Transformer based models. Some final thoughts are offered, and further reading is suggested for those interested.


\section{Goals For This Thesis}
This work focuses on providing a number of implementations of a Generative Chatbot, installed on small headless computers. Some overall goals are listed below. Checks in the check-boxes indicates the goal was achieved.

\begin{itemize}
	
	\item[\rlap{\raisebox{0.3ex}{\hspace{0.4ex}\tiny \ding{52}}}$\square$] Implement a generative chatbot using Recurrent Network components. This implementation uses GRU objects and the Tutorial found at Inkawhich et al \cite{2018Inkawhich}.
	
	\item[\rlap{\raisebox{0.3ex}{\hspace{0.4ex}\tiny \ding{52}}}$\square$] Implement a generative chatbot using a Transformer architecture and the Movie Dialog Corpus. This is discussed in Section \ref{transformer-movie-corpus}. %This implementation uses Scaled Dot Product attention.
	
	\item[\rlap{\raisebox{0.3ex}{\hspace{0.4ex}\tiny \ding{52}}}$\square$] Implement a chatbot using the Generative Pre-Trained 2 transformer. This is a pre-trained model. This is discussed in Section \ref{install-gpt2-chatbot}.
	
	\item[\rlap{\raisebox{0.3ex}{\hspace{0.4ex}\tiny \ding{52}}}$\square$] Subjectively compare the GRU, Transformer, and GPT2 models and the kind of output that each one produces. This is discussed throughout Section \ref{experiments-installations}.
	
	\item[\rlap{\raisebox{0.3ex}{\hspace{0.4ex}\tiny \ding{52}}}$\square$] Successfully install Google Speech Recognition and Text To Speech on the Raspberry Pi computer platform, as well as other small computers like the NVIDIA Jetson Nano. This is discussed in Section \ref{speech-and-speech-to-text}.
	
	\item[\rlap{\raisebox{0.3ex}{\hspace{0.4ex}\tiny \ding{52}}}$\square$] Install a GRU, Transformer, and GPT2 model on individual Raspberry Pi computers. Also install Google Speech Recognition and Speech To Text on these computers so that they can operate without keyboard or monitor. Allow the chatbot model to interact with a human's voice. This is discussed throughout Section \ref{experiments-installations}.
	
	\item[\rlap{\raisebox{0.3ex}{\hspace{0.4ex}\tiny \ding{52}}}$\square$] Install a GPT2 model on a Jetson Nano, along with Google Speech libraries and allow a human to interact with the chatbot. Compare execution time to the same model on the Raspberry Pi. This is discussed in Section \ref{chapter-nano}.
	
	\item[\rlap{\raisebox{0.3ex}{\hspace{0.4ex}\tiny \ding{52}}}$\square$] Compare the GRU and the Transformer model. Try to compare the word usage and sentence usage of the GRU and Transformer. This is discussed in Section \ref{gru-vs-transformer}.
	
	\item[\rlap{\raisebox{0.3ex}{\hspace{0.4ex}\scriptsize \ding{56}}}$\square$] Implement a generative chatbot using Recurrent Network components and an independent code base. This was not accomplished as the task was beyond the scope of this work.
	
	\item[\rlap{\raisebox{0.3ex}{\hspace{0.4ex}\scriptsize \ding{56}}}$\square$] Implement a generative chatbot using a Transformer architecture and the Persona chatbot corpus. This implementation did not work well. This is discussed in Section \ref{transformer-persona-corpus}. %uses Scaled Dot Product attention.
	
	
\end{itemize}

