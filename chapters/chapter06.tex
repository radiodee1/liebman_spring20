

\section{GRU vs. Transformer}
%It is important here to compare 
The GRU chatbot was compared with the larger Transformer based chatbot. Using subjective qualifications the GRU model answered with more variety than the Transformer model. An important observation was that the hyper-parameter set for the Transformer model could be expanded and enlarged as needed before training. The GRU model could not be trained successfully with an arbitrarily large hyper-parameter set. The larger Transformer could be trained to produce better responses.

Additionally, the GRU model responded very quickly, while the Transformer model took longer. %This is not a problem for general applications, but one cannot ignore 
The time spent by the Transformer model when installed on a small computer like a Raspberry Pi could not be ignored. 

%The respective value of each of the models changed depending on the platform they were implemented on. %The GRU responds more quickly.% and so it retains some worth.

Furthermore, time complexity is discussed by Vaswani et al \cite{Vaswani2017AttentionIA}. 
For the GRU Recurrent Neural Network it is $ n \cdot d^2 $, where $ n $ is the number of words in a sentence and $ d $ is the dimension of the hidden vector. Complexity for the Transformer's Scaled Dot Product Attention is $ n^2 \cdot  d $. Again, $n$ is the number of words and $d$ is the hidden vector's dimension. 

The maximum allowable input from the GRU is short, and in the example was 10. The maximum allowable input from the Transformer models varies but, in the 117M GPT2 model, it was 1024. 
%In the Transformer from Section \ref{transformer-movie-corpus} it might be 512. This is the same as the hidden vector size for the Transformer in question.

In this example, the hidden vector size for the GRU model was 500. The size for the 117M GPT2 model was 768, and the Transformer's hidden size was 512.

\section{Turing Test}

The Turing Test concerns itself with the question of whether a computer is intelligent. Turing says that intelligence is too hard to describe and that, if the computer can convince you that it is intelligent, then it is.

Whether this is right is beyond the scope of this thesis. The developers of the Generative Pre-Training 2 transformer were apprehensive about their model's ability to generate human text and felt the model worked too well. 
At first they decided not to release the largest version to the public for several months. 
%(Radford et al) \cite{radford2019language} but ultimately did release it. 
%The model developers used it differently than our chatbot implementation. 
%They generated paragraphs of text, and it was determined at first that the ability of the model to impersonate a human was too great. 
They felt that the model could be used to spam Facebook and other social networking sites with content that was very convincing. 
%If the model could be used to convince people to act badly, then it should not be released. 
%Humans are susceptible to the sentiments of those they see as their peers. 
%If these models could, for better or worse, passing the Turing test, then it should not fall into the wrong hands. 
%This was the concern of the coders at the time.

Ultimately the large model was released, either because the developers decided the model was not as good as originally estimated. %, or because they didn't care. 

\section{Word and Sentence Comparisons}

It is important to understand the training and inference stages. Several graphs and a discussion of the GRU model and the Transformer model and their word and sentence usage follows.

The fully trained GRU and Transformer models are setup in inference mode and their output is observed when they are exposed to the first 10,000 training question sentences from the Movie Subtitles corpus.

\subsection{Word Usage}

In general the Transformer model uses only a small subset of words that it has available to it. In the example below, Figure \ref{diagram-words-with-voc-total}, 10,000 input lines were tested from the training set. A small percentage of the vocabulary words were used in the output. %These sentences came from the movie dialog corpus. Some percentage of words are used repeatedly.

\begin{figure}[H]
	\begin{center}
		\includegraphics[scale=0.75]{diagram-output-total-responses-01}
		
		
	\end{center}
	\caption[Word Usage]{Word Usage - Including Vocabulary Total}
	\label{diagram-words-with-voc-total}
	
\end{figure}

The green line at the top of Figure \ref{diagram-words-with-voc-total} is the total words in the GRU vocabulary. %The total words in the GPT2 vocabulary is not represented on the graph. 
Total responses in words are shown with the three remaining lines. The GPT2 responses are higher, and the GRU and Transformer responses look like horizontal lines at the bottom of the graph.

In Figure \ref{diagram-words-no-voc-total}, if the Vocabulary Total and the GPT2 responses are removed, the outputs take the shapes of a curves with a maximum of around 475 words. The Transformer model has a vocabulary size of 8170 tokens, and the GRU model is close to that at 7826 tokens. %Both models use the same training corpus. The difference between the tokens available and the tokens used is large.


\begin{figure}[H]
	\begin{center}
		\includegraphics[scale=0.75]{diagram-output-total-responses-02}
		
		
	\end{center}
	\caption[Total Word Responses]{Total Word Responses - No Vocabulary Total}
	\label{diagram-words-no-voc-total}
	
\end{figure}

It is possible the GRU model operates more robustly than the Transformer model, or the Transformer model may be over trained or over fitted. It also might be that the hyper parameter set is poorly adjusted. The learning rate, for example, may be too high.

\subsection{Sentence Usage}
A goal for this work included answers to these questions.
How many fully formed responses does the given model use? Additionally, when or how many of these responses are used repeatedly?

While the GPT2 model is very large and very versatile, the Transformer and GRU models are smaller. %For these two, smaller models it would be good to tell how many repeated sentences occurred as output in some number of inputs. 
In Figure \ref{diagram-words-limit-shown}, 10,000 inputs from the training set of the movie dialog corpus were used. % in contrast to the previous section where 2000 inputs are used.

%There are rough numbers for those two models. 
These models use less than 350 sentences repeatedly. At the end of the study, 900 total sentences were used by the GRU. At some point the number of total GRU sentences that the model can produce will reach a limit below the vocabulary total.

\begin{figure}[H]
	\begin{center}
		\includegraphics[scale=0.75]{diagram-output-repeated}
		
		
	\end{center}
	\caption[Simple Sentence Usage]{Simple Sentence Usage}
	\label{diagram-words-limit-shown}
	
\end{figure}

While the Transformer and GRU models have similar `Repeated Sentence' graphs, they are different models and have different sizes. 
%They are not similar in many ways but in this metric. 
However as they share the same corpus, they may be learning the same task.

\section{Training and Lists}
%In this discussion the output of the GPT2 model is mostly ignored. %There is a short discussion of the Transformer and the GRU.

%It is important to know what the Transformer or GRU was doing during training and later during inference. 
Here the Transformer and GRU models are trained on the movie dialog corpus. The Transformer model has 6 layers, 8 heads, and a hidden size of 512 units, while the GRU model has a hidden size of 500.

Initially the model learns a set of multi-purpose English answers. % in a form close to a list. 
Then it acts as a classifier, where each input sentence is compared to the set of answers. 
%A question is associated with a given answer from the set when possible. 
There would be fewer answers than there are questions. 
%There may be a list that contains many of the answers the model would use. 
%For a given model this list can only be so long.

At the start the multi-purpose answers are constructed at the same time that the classification task is taking place. 

%This list making is a function of the model trying to optimize the answers that it gives, and can give, given a certain size memory capacity. 
The model has a certain capacity and it starts to develop lists of usable answers in order to use that capacity best.

For the GPT2, full sentences may not be saved because the model may be more dynamic. For translation, for instance, it may be able to remember longer lists. In place of a list of complete responses, a list of phrases or partial responses could be combined to create translated output.

The output of the Transformer or GRU shows little intelligence. The actual utterances of the model are plain. The internal building of lists, though, may show a process that is found in some intelligent activity.