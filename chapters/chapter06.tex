

\section{GRU vs. Transformer}
%It is important here to compare 
The GRU chatbot was compared with the larger Transformer based chatbot. Using subjective qualifications the GRU model answered accurately and with more variety than the Transformer model. An important observation was that the hyper-parameter set for the Transformer model could be expanded and enlarged as needed before training. The GRU model could not be trained successfully with an arbitrarily large hyper-parameter set. The larger Transformer could be trained to produce better responses.

Additionally, the GRU model responded very quickly, while the Transformer model took longer. %This is not a problem for general applications, but one cannot ignore 
The time spent by the Transformer model when installed on a small computer like a Raspberry Pi could not be ignored. 

%The respective value of each of the models changed depending on the platform they were implemented on. %The GRU responds more quickly.% and so it retains some worth.

Furthermore, time complexity is discussed by Vaswani et al \cite{Vaswani2017AttentionIA}. 
For the GRU Recurrent Neural Network it is $ n \cdot d^2 $, where $ n $ is the number of words in a sentence and $ d $ is the dimension of the hidden vector. Complexity for the Transformer's Scaled Dot Product Attention is $ n^2 \cdot  d $. Again, $n$ is the number of words and $d$ is the hidden vector's dimension. 

The maximum allowable input from the GRU is short, and in the example was 10. The maximum allowable input from the Transformer models varies but, in the 117M GPT2 model, it was 1024. 
%In the Transformer from Section \ref{transformer-movie-corpus} it might be 512. This is the same as the hidden vector size for the Transformer in question.

In this example, the hidden vector size for the GRU model was 500 and %. The size for the 117M GPT2 model was 768, and 
the Transformer's hidden size was 512.

\section{Turing Test}

The Turing Test concerns itself with the question of whether a computer is intelligent. Turing says that intelligence is too hard to describe and that, if the computer can convince you that it is intelligent, then it is.

Whether this is right is beyond the scope of this thesis. The developers of the Generative Pre-Training 2 transformer were apprehensive about their model's ability to generate human-like text and felt the model worked too well. 
At first they decided not to release the largest version to the public for several months. 
They felt that the model could be used to spam Facebook and other social networking sites with content that was very convincing. 


Ultimately the large model was released, possibly because the developers decided the model was not as good as originally estimated. %, or because they didn't care. 

\section{Word and Sentence Comparisons}

It is important to understand the training and inference stages. Several graphs and a discussion of the GRU model and the Transformer model and their word and sentence usage follows.

The fully trained GRU and Transformer models are set up in inference mode and their output is observed when they are exposed to the first 150,000 training question sentences from the Movie Subtitles corpus.

All the graphs start out with one of the network models and a group of 150,000 input sentences. With every change of x all the sentences up until that index are fed to the model. Then they are processed by whether or not they show up repeatedly in the output so far. Then the output is summed. The changes in y show this summed output.

\subsection{Word Usage}

In general the Transformer model uses only a small subset of words that it has available to it. In the example below, Figure \ref{diagram-words-with-voc-total}, 150,000 input lines were tested from the training set. A small percentage of the vocabulary words were used in the output. %These sentences came from the movie dialog corpus. Some percentage of words are used repeatedly.

\begin{figure}[H]
	\begin{center}
		\includegraphics[scale=0.75]{diagram-output-total-responses-05}
		
		
	\end{center}
	\caption[Word Usage]{Word Usage - Including Vocabulary Total}
	\label{diagram-words-with-voc-total}
	
\end{figure}

The blue line at the top of Figure \ref{diagram-words-with-voc-total} is the total words in the GPT2 vocabulary. It is much higher than the GRU or Transformer vocabulary.
The green line is the total words in the GRU vocabulary. Total responses in words are shown with the three remaining lines. The GPT2 responses, in red, are higher, and the GRU and Transformer responses look like horizontal lines at the bottom of the graph.

In Figure \ref{diagram-words-no-voc-total}, if the Vocabulary Total and the GPT2 responses are removed, and 150,000 sentences are tested, the outputs take the shapes of curves with a maximum of around 1,100 words. The Transformer model has a vocabulary size of 8170 tokens, and the GRU model is close to that at 7826 tokens. Both curves are slowly increasing at this time.


\begin{figure}[H]
	\begin{center}
		\includegraphics[scale=0.75]{diagram-output-total-responses-04}
		
		
	\end{center}
	\caption[Total Word Responses]{Total Word Responses - No Vocabulary Total}
	\label{diagram-words-no-voc-total}
	
\end{figure}

It is possible the GRU model operates more robustly than the Transformer model, or the Transformer model may be over trained or over fitted. It also might be that the hyper parameter set is poorly adjusted. The learning rate, for example, may be too high.

\subsection{Sentence Usage}
A goal for this work included answers to these questions.
How many fully formed responses does the given model use? Additionally, when or how many of these responses are used repeatedly?

While the GPT2 model is very large and very versatile, the Transformer and GRU models are smaller. In Figure \ref{diagram-words-limit-shown}, 150,000 inputs from the training set of the movie dialog corpus were used. 

These models use less than 2,000 sentences repeatedly. At the end of the study, 4,000 total sentences were used by the GRU. We feel that at some point the number of total GRU sentences that the model can produce will reach a limit below the vocabulary total.

\begin{figure}[H]
	\begin{center}
		\includegraphics[scale=0.75]{diagram-output-repeated-02}
		
		
	\end{center}
	\caption[Simple Sentence Usage]{Simple Sentence Usage}
	\label{diagram-words-limit-shown}
	
\end{figure}

If the graphs of the repeated sentences changed vertically as they do horizontally, the lines would be shown to increase very slowly.

While the Transformer and GRU models have similar `Repeated Sentence' graphs, they are different models and have different sizes. 
However as they share the same corpus, they may be learning the same task.

\subsection{Maximum Sentence Values}

For each of the models there is only a certain number of tokens in the model's vocabulary. Additionally there are only so many tokens allowed in the output. For the GRU and the Transformer the output is limited to 10 tokens. For the GPT2 model there is a limit of 1024 tokens. There are only so many combinations that can be formed for a given model.

It seems that the models cannot use all of the words or tokens in their vocabulary. They can identify a number of them that they use repeatedly. This would mean the possible combinations that a model could produce is smaller.

\section{Training and Lists}

Here the Transformer and GRU models are trained on the movie dialog corpus. The Transformer model has 6 layers, 8 heads, and a hidden size of 512 units, while the GRU model has a hidden size of 500.

Initially the model learns a set of multi-purpose English answers. Then it acts as a classifier, where each input sentence is compared to the set of answers. There would be fewer answers than there are questions. 

At the start the multi-purpose answers are constructed at the same time that the classification task is taking place. 


The model has a certain capacity and it starts to develop lists of usable answers in order to use that capacity best.

For the GPT2, full sentences may not be saved because the model may be more dynamic. For translation, for instance, it may be able to remember longer lists. In place of a list of complete responses, a list of phrases or partial responses could be combined to create translated output.

The output of the Transformer or GRU shows little intelligence. The actual utterances of the model are plain. The internal building of lists, though, may show a process that is found in some intelligent activity.