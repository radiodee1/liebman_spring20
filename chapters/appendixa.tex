\chapter{Further Installations}

\section{Generative Pre-training Transformer 2 - Large Model}

The large 774M model of GPT2 was ultimately released to the public. The model is too large for a Raspberry Pi, using 6.3 Gig of ram while it is running with inference. We experiment with the model on the development laptop computer.

As with the smaller model and the Raspberry Pi installation, we do not attempt transfer learning on the model. Instead we show the model specific information during inference which the model is free to reply with if it finds the information appropriate.

We also want the model to be able to summarize articles from the internet and reply with that information when asked to do so. 

\subsection{Context Experiment}

As before we gave the model details in sentence form that it could use as reply to questions when appropriate. The larger model works better if all the sentences shown to it have the `Q:' and `A:' strings prepended to them. 

In each example the information we wanted the model to choose from was presented in sentence pairs. Though these sentences look much like source/target pairs from our movie corpus, they are totally unrelated.

An example might be the presentation to the model of the model's name. In the smaller GPT2 model we simply show the model the single sentence `My name is Jane'. In the larger GPT2 model's code we must show the model two sentences. One is `Q: What is your name?' and the other one, following it directly, is `A: My name is Jane'. We follow these conventions for the time of day, the model's location, and the model's occupation.

These sorts of questions can be answered without any memory of what the last question was.

\subsection{History Experiment}

We tried concatenating all input and output and including it with each question. While this did little or nothing during the smaller GPT2 experiment, with the larger model the history sentences were helpful. Consider the questions below.
\begin{verbatim}
> Do you like the color red?
I like the color red.
> What is your favorite color?
Red.
\end{verbatim}
This is the same text excerpt as from the small GPT2 model. The `favorite color' example continues to work on the larger GPT2 model. Again as in the small model, the chatbot is keeping track of the conversation. 

Even though the history is consulted before answers are formulated, the output of the model is not always predictable. This has something to do with the `temperature' setting of the model. The output is sometimes more fanciful.


\subsection{Artificial Intelligence Markup Language Experiment}

We want to be able to tell the model exactly how to answer a given question if we have a particular need. For example, the model handles remembering the user's name very badly. We would like to dictate how the model handles that situation. We have employed Artificial Intelligence Markup Language for this. 

AIML is actually poorly suited for handling this task. In AIML you must code every question and response as a separate rule. If you have a question `How are you?' and an answer `I am fine' the AIML kernel will only answer with the programmed response if the input is an exact match. If you enter `How are you doing?' or `How do you feel?' the kernel does not have an answer for that. The two or three questions are phrased differently but can be answered with the same text.

The GPT2 chatbot is much different. Groups of questions have the same answer. Sometimes the answer could even be `I don't know', but still the answer makes sense in English. The chatbot could answer `How are you?', `How are you doing?' and `How do you feel' with one answer like `I don't know' or `I am fine'. These questions are phrased differently with the same answer.

For us to use AIML for our task we code questions and answers in pairs for all possible phrasing of a question.

We have to count the number of AIML rules that we are coding and compare it to the number of English sentences that the GPT2 chatbot can answer. We have to make sure that the number of chatbot answers is much higher. 

We are interested in two numbers. One is the average number of rules that we write in AIML to represent a given outcome. The other number is how many outcomes we are working on.

We prefer the answers provided by the generative chatbot over those provided by an AIML file. We want to maximize the amount of time that the overall model answers using the transformer that is internal to the chatbot, and minimize the use of AIML and the AIML kernel. We set a criteria for ourselves. If the generative chatbot is handling most of the questions and answers then we can proceed. If the AIML question/answering set is larger than the number of questions that the chatbot can answer, then the AIML should be avoided. 

\subsubsection{User Name}
The name of the user is our first example of AIML coding. This example is easy. There are few ways to say `My name is *'. In this case `*' is a wildcard. Another variant is `* is my name'. That's as many ways to say it as there are.

Each question is answerable with a phrase like `Hello *'. Here the asterisk is a wildcard for the memorized name. An AIML file can handle this sort of question very easily. We write an AIML file with these questions. We depend on the AIML to remember the name from the wildcard. 

Another question in this scenario is `What is my name?' With this question also there are very few ways to phrase the AIML. Here we are referring to the name recorded in the previous AIML question as discussed above. 

%We have a situation where we are working on two outcomes. One is to tell the chatbot your name. The second is to ask the chatbot to tell you what that name is, albeit from the earlier question. For the time being we will add together the number of phrases in both questions. For this example we record three phrases. Two phrases come from stating your name, and one phrase comes from the answer to the question `What is my name?'

During a typical chatbot run we show all user input to the AIML kernel. 

When the kernel does not match anything there is no output from the kernel, but when there is a match we show the AIML output to the chatbot as the context for that input. This is to say that the input from the user is concatenated with the context text, in this case the answer from the AIML kernel. The chatbot is then left to decide what to answer from the user question and the context. Usually the answer makes sense.

Usually the user's question and whatever the chatbot decides will be the answer are both added to the history. As noted before, history of what the user enters and the chatbot's answer are kept by the chatbot so that the chatbot can keep track of answers over time. Sometimes, though, keeping sentences that refer to user names confuses the bot's answers. In other cases the bot keeps track of answers that are generated in the AIML/chatbot hybrid process.

\subsubsection{Internet Search Experiment}

We want to be able to search the internet for an article, download that article, and summarize parts of the article as answers to questions asked of the model. This is leveraging of the token size of the input to the model. It's usually very large. It can be 1024 tokens.

The AIML required is minimal but larger than the AIML for the `user name' problem. The goal is to detect when the user wants to talk about an internet page and detect when they are done.

The first case, when the user wants to talk about an internet page, is associated with utterances like `Tell me about *' and `I want to talk about *' where `*' is a wildcard for what the user is interested in. There can be other phrases that imply the same intention on the part of the user. In these cases the text from the wildcard in the invocation is saved.

The second case, when the user is done with the internet page and wants to return to normal operation, is associated with utterances like `Ok thanks' and `That's enough'. These two phrases can be checked for when the bot is in `Internet-Search' mode. There could easily be other phrases that cause the same operation in the chatbot code.

When the AIML detects one of these signals the program goes into a special mode of operation. The first signal, we will call the `find' signal, causes operation of an internet browser. The web search uses the text from the invocation of the `find' command. The browser yields results as they would be found using a google search engine. The first 20 results are kept and the url for the highest ranking wikipedia page is retained. That page is loaded and the `body' tag is scanned for `paragraph' type content. This web page replaces the context input. 

After that the context is retained while the chatbot answers any questions that the user might have. The GPT2 model has some facility with this, and the larger GPT2 model is more adept.

Later when the AIML detects the second signal, we will call the `restore' signal, the program switches back from special operation to normal operation. The web page contents are removed and the context is restored as it was during regular chatbot use. There is a part that has the history of the last few questions and answers, and there is a part that has the bot name, occupation, and the current time.

As the chatbot answers questions the content of the conversation is saved for the `history' of the context text, as it is used in the regular chatbot mode. This means there is a record of the questions and answers the chatbot goes through during it's operation, even when it is answering questions about a web page that it has searched for. Because of size limitations the history is not included in the chatbot input when it is answering questions about a web page, but a record is kept of `q' and `a' and the bot can refer to them later.

Again, because of size restrictions there is no regular context information shown to the chatbot while it is answering questions about a web page. Then, after the `restore' signal, the chatbot sees only the regular context and none of the web page material. The regular input for the small model is 768 tokens and it is 1280 for the larger model. Though this would appear to be a large space for text, it is common for wiki articles to be clipped so that the user's question can be inserted at the end.

\subsubsection{Usage Example}

Consider that you are interested in the musical group `The Beatles'. If you wanted to find out about the band you might say to the chatbot `Tell me about the beatles.'

The chatbot would take a little time to respond. Ultimately it would answer with something like `The Beatles were an English rock band formed in Liverpool in 1960.'

Then you might ask it a question like `How many members were in the band?'

To this question the chatbot answered this way for us, `The Beatles were originally formed by John Lennon and Paul McCartney in Liverpool.' We note that this is not actually the correct answer. This may be because after clipping the information asked for does not exist in the context section. You can ask other questions. Some of them may be answered correctly and some not. 

When you ask the question `How many members were in the band?' you do not need to specify which band. The model seems to be able to intuit the band that you are referring to.

At some point you will want to end the Q/A session. You would say `OK' or `OK thanks'. After that the web page for the `Beatles' would no longer be available to the chatbot. It could still answer questions. You could ask `What are the Beatles'. You might get an answer like above, `The Beatles were an English rock band formed in Liverpool in 1960.'

This is an example of using the model. Other topics can be asked about. You do not need to re-start the model to ask about a second or third topic. Each question takes several seconds before an answer is produced.