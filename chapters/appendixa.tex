\chapter{Terminology}



\iffalse
We use a Gated Recurrent Unit arrangement for a Sequence to Sequence chatbot.

Below is a product formula. It says that the $y_n$ term relies on the sequence of terms that starts with $y_1$ and continues to $y_{n-1}$. It also has $v$ that represents the 'thought vector' which is present along with $y_1,...,y_{t-1}$ as an input to the decoder Gated Recurrent Unit and helps formulate $y_t$.

On the left side of the formula we have $x_1,...x_T$ which represents the input sequence for the encoder. Also we have $y_1,...,y_{T'}$ which represents the entire output. $T$ is the length of the input while $T'$ is the length of the output. Those two $T$ terms do not need to be the same, and often are not. Again, $v$ is the hidden state passed from the encoder to the decoder as a 'thought vector'.

\[
\mathlarger{ \mathlarger{ \mathlarger{
		p(y_1,...,y_{T'}|x_1,...,x_T) = \prod_{t=1}^{T'} p(y_t|v,y_1,..., y_{t-1}) 
} } }
\]

Every prediction relies on all the previous predictions up until that point. This is what happens in sequence to sequence and transformer architectures when the model is producing output. This function can be found in Sutskever et al (2014)\cite{DBLP:journals/corr/SutskeverVL14}.
\fi

\iffalse
Below is a diagram of the sequence to sequence architecture.

\begin{figure}[H]

\begin{center}

\includegraphics[scale=0.5]{diagram-nmt}

	
\end{center}

\caption[Sequence to Sequence]{Seq2seq - A and B represent an input sequence and C and D represent the corresponding output.}

\end{figure}


\section{Sequence to Sequence Architecture}

In Fig. A.1 we generalize a sequence to sequence model. The idea is that A and B, on the left side of the diagram, deal with the encoding of sentences. A and B would be consecutive words in a sentence, and the round blue nodes below A and B are Recurrent Neural Network units. C and D are outputs and in the right side of the diagram the blue nodes represent the output Recurrent Neural Network units. Between the input and the output there is a corridor of information exactly the size of the Recurrent Neural Network hidden vector. 

All of the information that the decoder uses for it\textquoteright s output is present in this corridor and is passed along the corridor from the encoder. For this reason we refer to it as the thought vector.

Making this vector larger by increasing the size of the hidden unit size allows for more information in the thought vector. Size also increases the time to train the network. The size must also match the dimension of the vectors in the GloVe or Word2Vec download if
one of those is used. 

Ultimately exceedingly large hidden dimension does not improve the sequence to sequence model. 
\fi

\iffalse
\section{Corpus Considerations}
We have collected several data sets for the training of a chatbot model. Firstly we have a corpus of movie subtitles. Secondly we have the `JSON' dump from Reddit that is downloadable. Finally we have the corpus described by Mazar{\'{e}} et al(2018)\cite{DBLP:journals/corr/abs-1809-01984}. This final corpus is designed for training the chatbot task specifically. This is referred to as the Persona corpus.

The movie corpus is medium sized and the Reddit `JSON' download is large
and filled with hyperlinks and sentence fragments. 

At the time of
this writing we are using the movie subtitles corpus and the Persona corpus. We use the movie
corpus because it is smaller. Both the movie corpus and the Reddit
corpus are described as noise filled, so it is likely that neither
one is perfect for the training. The movie corpus is easier to deal
with if we are training on a single processor. In the future if we
can train in a faster environment the Reddit corpus might be superior.

For the Persona corpus the text is organized into `JSON' objects. There
are several different repeated labels. Some of the text is meant to be used in question and answer pairs. There is also some very specific information there that is not
organized in this kind of pattern. When we take apart the Persona corpus
we find that the sentences labeled with the `history' tag are most suited to our task.
We record these values only and discard other labels.
\fi

\iffalse
\section{Word Embeddings}

There are several basic building blocks of sequence to sequence models. They are regular neural network weights and biases, recurrent network components, and word embedding components.

Regular neural network cells, are arranged in layers and are pretty straight forward in most programming environments. To use them you define their dimensions in width and height. They have weights and biases that must be initialized before use.

Recurrent networks have internal parts that are constructed of regular network cells, so they have weights and biases too. They have several internal dimensions that need to be set. One of these is the RNN hidden unit. This hidden unit is a dimension.

Word embedding components are the third item we want to describe. What happens is words are translated from strings to individual numbers from a vocabulary dictionary. The dictionary only contains a single unique number for every word. Then the number is passed through an embedding structure. This turns the single number into a vector of numbers that is the same size as the RNN hidden dimension. Then, from that time on the model uses the vector instead of words.

The contents of the embedding unit is a table of numbers, all of the size of the RNN hidden dimension. The vectors are usually, but don\textquoteright t have to be, unique values. There is one complete hidden dimension sized vector for each word in the vocabulary. 

\begin{figure}[H]
	\begin{center}
	\includegraphics[scale=0.5]{diagram-embedding}
	
	
\end{center}
	\caption[Word Embeddings]{Embeddings - Each word from a dictionary is converted to a vector of numbers.}
	
	%\addcontentsline{lof}{section}{Word Embeddings}
\end{figure}

The vectors can be initialized randomly or they can be filled with predetermined values. As the network trains the embedding values can either be modified or frozen in place. Typically if the contents were initialized randomly the values would be trained. If the contents were filled with predetermined values you don\textquoteright t want to train them or change them in any way. 

There are at this writing two main types of pretrained word embeddings. One is called \textquoteleft Word2Vec\textquoteright{} and one is called \textquoteleft GloVe\textquoteright . 

Word2Vec is short for \textquoteleft Word to Vector.\textquoteright{} (Mikolov et al, 2013)\cite{mikolov2013efficient} GloVe is short for \textquoteleft Global Vector.\textquoteright{} (Pennington et al, 2014)\cite{pennington-etal-2014-glove} .
\fi

\iffalse
\section{WordPiece - Byte Pair Encoding}

\ac{BPE} stands for `Byte Pair Encoding.' WordPiece is a particular implementation of Byte Pair Encoding.

WordPiece is used by the BERT system to encode words much the way that Word2Vec does. Like Word2Vec, WordPiece  has a vocabulary list and a table of embeddings that maps one word or token to a vector of a given size.

WordPiece, though, handles Out Of Vocabulary (OOV) words gracefully. It breaks large words into smaller pieces that are in the vocabulary, and has a special notation so that these parts can easily be recombined in order to create the input word again.
\fi

\section{Transformer}

In this section we discuss the `Transformer' model. In their paper, Vaswani et al (2018)\cite{tensor2tensor} describe use of the python project Tensorflow and the Transformer model which is part of it.

The Transformer uses no recurrent elements. It is in a sense a group of attention mechanisms. The Transformer in this case is a single structure that can be used to solve many machine learning problems. It is used for Neural Machine Translation, Sentiment Analysis, and others. It is a single model capable of solving many machine learning questions.

We use the translation models for the chatbot problem by feeding the model the English language on both input and output. 

The Transformer itself can be configured for sentence long output but it is not a pre-trained model. There are pre-trained versions of the transformer, one of which is called BERT. BERT is described in Devlin et al (2018)\cite{DBLP:journals/corr/abs-1810-04805} and the acronym stands for Bidirectional Encoder Representations from Transformers. Unfortunately BERT output is as a 
classifier. Full sentence-length output is not supported.

\section{Generative Pre-training Transformer 2}

Another pre-trained model that uses the Transformer is GPT2. GPT2 stands for `Generative Pre-training Transformer 2.'

GPT2 is a model that takes as input a seed sentence or topic and returns as output text in the same language that is auto-generated. GPT2 also is very capable at summarizing the input or seed statement. We use both of these capabilities in our experiments.

There are two GPT2 models. One is larger than the other. The smaller model has been released and the larger model has not. 

GPT2 is discussed in Radford et al (2019)\cite{radford2019language} and on the blog associated with the parent company, OpenAI.com. (https://openai.com/blog/better-language-models/)

GPT2 comes with its own vocabulary encoding and decoding functionality. This system is closer to
WordPiece and BPE than it is to Glove or Word2Vec.

It is pre-trained on a corpus that is developed from Reddit posts called WebText. WebText is a
40 Gigabyte corpus that takes high powered computers to train.

The version of GPT2 which has been released is similar in size to the largest currently released 
BERT transformer model. GPT2 has been released in Tensorflow format, and more recently in a converted PyTorch format. It is possible to download the trained model and then fine-tune the model for your own task. This kind of fine-tuning is called `transfer learning'. 

GPT2 works so well in certain conditions that it is appropriate to use without fine tuning. This 
type of implementation is called `zero-shot' implementation. We use GPT2 in a zero-shot implementation with the chatbot problem.


\iffalse
\section{Raspberry Pi}

A Raspberry Pi is a small single board computer with an `arm' processor. There are 
several versions on the market, the most recent of which sports built-in wifi and
on-board graphics and sound. The memory for a Raspberry Pi 3B computer is 1Gig of RAM. Recently
available, the Raspberry Pi 4B computer can sport 4Gig of RAM.

It has always been the intention that at some time some chatbot of
those examined will be seen as superior and will be installed and
operated on a Raspberry Pi computer. If more than one model is available
then possibly several models could be installed on Pi computers.

For this to work several resources need to be made available. Pytorch
needs to be compiled for the Pi. Speech Recognition (\ac{SR}) and Text
To Speech (TTS) need to work on the Pi.

For the transformer model to work Tensorflow needs to work on the Pi.

All the files that are trained in the chosen model need to be small
enough in terms of their file size to fit on the Pi. Also it must
be determined that the memory footprint of the running model is small
enough to run on the Pi.

In the github repository files and scripts for the Raspberry Pi are
to be found in the \textquoteleft bot\textquoteright{} folder.

Early tests using Google\textquoteright s SR and TTS services show
that the Pi can support that type of functionality. 

Google's SR service costs money to operate. Details
for setting up Google's SR and TTS functions is beyond
the scope of this document. Some info about setting this up can be
found in the README file of this project\textquoteright s github
repository.

The pytorch model that is chosen as best will be trained on the
desktop computer and then the saved weights and biases will be transferred
to the Raspberry Pi platform. The Pi will not need to do any training,
only inference. 
\fi

\iffalse
\section{Tensorflow vs. Pytorch}

Tensorflow is a Google library. Pytorch has it's roots with Facebook. Both run in a Python environment. The learning curve for Tensorflow is steeper than for Pytorch. Pytorch offers the programmer python objects that can be combined to create a neural network. Tensorflow has different pieces that can be combined, but they cannot be examined as easily at run time.

Tensorflow has a placeholder concept for inputting data and getting back results. You set up these placeholders at design time. They are the only way of accessing your data at run time.

Pytorch objects interact with Python more naturally. You can use print statements in your code to show data streaming from one object to another. This is possible at run time.

In favor of Tensorflow, it has a good tool for visualization which can print out all kinds of graphs of your data while your model trains. It is called Tensorboard.

\fi
