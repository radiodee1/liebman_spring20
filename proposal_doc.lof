\babel@toc {english}{}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Recurrent Neural Network}}{3}{figure.1.1}% 
\contentsline {figure}{\numberline {1.2}{\ignorespaces Word Embeddings}}{6}{figure.1.2}% 
\contentsline {figure}{\numberline {1.3}{\ignorespaces Sequence to Sequence Architecture}}{9}{figure.1.3}% 
\contentsline {figure}{\numberline {1.4}{\ignorespaces Loss and Accuracy}}{13}{figure.1.4}% 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Transformer Encoder and Decoder}}{18}{figure.2.1}% 
\contentsline {figure}{\numberline {2.2}{\ignorespaces Lowering Dimensionality}}{19}{figure.2.2}% 
\contentsline {figure}{\numberline {2.3}{\ignorespaces Attention Output}}{20}{figure.2.3}% 
\contentsline {figure}{\numberline {2.4}{\ignorespaces Matching Input and Output}}{21}{figure.2.4}% 
\contentsline {figure}{\numberline {2.5}{\ignorespaces Transformer Encoder and Decoder Flow}}{22}{figure.2.5}% 
\contentsline {figure}{\numberline {2.6}{\ignorespaces Decoder Flow}}{23}{figure.2.6}% 
\contentsline {figure}{\numberline {2.7}{\ignorespaces Decoder Mask}}{24}{figure.2.7}% 
\contentsline {figure}{\numberline {2.8}{\ignorespaces Visualized Attention}}{25}{figure.2.8}% 
\contentsline {figure}{\numberline {2.9}{\ignorespaces Generative Pre-training Transformer 2 }}{27}{figure.2.9}% 
\contentsline {figure}{\numberline {2.10}{\ignorespaces Visualized Attention GPT2}}{30}{figure.2.10}% 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Raspberry Pi}}{36}{figure.3.1}% 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Loss - Larger Transformer Model}}{50}{figure.4.1}% 
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {6.1}{\ignorespaces Word Usage}}{67}{figure.6.1}% 
\contentsline {figure}{\numberline {6.2}{\ignorespaces Total Word Responses}}{68}{figure.6.2}% 
\contentsline {figure}{\numberline {6.3}{\ignorespaces Simple Sentence Usage}}{69}{figure.6.3}% 
\addvspace {10\p@ }
\addvspace {10\p@ }
