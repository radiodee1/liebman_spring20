\babel@toc {english}{}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Recurrent Neural Network}}{3}{figure.1.1}% 
\contentsline {figure}{\numberline {1.2}{\ignorespaces Word Embeddings}}{6}{figure.1.2}% 
\contentsline {figure}{\numberline {1.3}{\ignorespaces Sequence to Sequence Architecture}}{9}{figure.1.3}% 
\contentsline {figure}{\numberline {1.4}{\ignorespaces Loss and Accuracy}}{13}{figure.1.4}% 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Lowering Dimensionality}}{18}{figure.2.1}% 
\contentsline {figure}{\numberline {2.2}{\ignorespaces Attention Output}}{19}{figure.2.2}% 
\contentsline {figure}{\numberline {2.3}{\ignorespaces Matching Input and Output}}{20}{figure.2.3}% 
\contentsline {figure}{\numberline {2.4}{\ignorespaces Transformer Encoder and Decoder Flow}}{21}{figure.2.4}% 
\contentsline {figure}{\numberline {2.5}{\ignorespaces Transformer Mask}}{22}{figure.2.5}% 
\contentsline {figure}{\numberline {2.6}{\ignorespaces Transformer Encoder and Decoder}}{23}{figure.2.6}% 
\contentsline {figure}{\numberline {2.7}{\ignorespaces Visualized Attention}}{24}{figure.2.7}% 
\contentsline {figure}{\numberline {2.8}{\ignorespaces Generative Pre-training Transformer 2 }}{26}{figure.2.8}% 
\contentsline {figure}{\numberline {2.9}{\ignorespaces Visualized Attention GPT2}}{29}{figure.2.9}% 
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Loss - Larger Transformer Model}}{47}{figure.4.1}% 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces Word Usage}}{57}{figure.5.1}% 
\contentsline {figure}{\numberline {5.2}{\ignorespaces Simple Word Usage}}{58}{figure.5.2}% 
\contentsline {figure}{\numberline {5.3}{\ignorespaces Simple Sentence Usage}}{59}{figure.5.3}% 
\addvspace {10\p@ }
\addvspace {10\p@ }
